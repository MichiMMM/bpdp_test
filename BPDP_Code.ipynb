{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# We import the neccessary packages in the beginning\n",
    "import datetime\n",
    "import os\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statistics import mean,stdev\n",
    "import graphviz\n",
    "import time \n",
    "import pm4py\n",
    "#from pm4py.algo.filtering.log.variants import variants_filter\n",
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "from pm4py import conformance_diagnostics_alignments as alignments\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_based_replay\n",
    "#from pm4py.algo.filtering.log.auto_filter.auto_filter import apply_auto_filter\n",
    "from pm4py.statistics.attributes.log.get import get_attribute_values\n",
    "from pm4py.statistics.attributes.log.get import get_all_trace_attributes_from_log\n",
    "from pm4py.statistics.attributes.log.get import get_all_event_attributes_from_log\n",
    "from pm4py.statistics.attributes.log.get import get_trace_attribute_values\n",
    "from pm4py.objects.log.util import dataframe_utils\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.conversion.bpmn import converter as bpmn_converter\n",
    "from sklearn import linear_model\n",
    "from sklearn import tree\n",
    "from sklearn.impute import SimpleImputer\n",
    "#from pm4py.visualization.petri_net import visualizer as pn_visualizer\n",
    "import statsmodels.api as sm\n",
    "#import pygraphviz\n",
    "from pm4py.objects.log.obj import EventLog, Trace, EventStream\n",
    "#Causality Toolbox\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint, choice\n",
    "import copy\n",
    "import random\n",
    "import itertools\n",
    "import pkgutil\n",
    "from scipy.stats import pearsonr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import tqdm\n",
    "import time\n",
    "import xgboost as xgb"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Returns a path to the file selected by the user\n",
    "# Input: The folder in which to look for the files - the default is the current folder\n",
    "def ask_for_path(rel_path='', index = -1):\n",
    "    #Crawl all files in the input folder\n",
    "    print(\"The following files are available in the input folder:\\n\")\n",
    "\n",
    "    count = 0\n",
    "    file_list = os.listdir(os.getcwd() + rel_path)\n",
    "    for file in file_list:\n",
    "        print(str(count) + \" - \" + file)\n",
    "        count+=1\n",
    "\n",
    "    if(index == -1):\n",
    "        #Ask for which of the files shall be transformed and select it.\n",
    "        inp = input(\"Please choose from the list above which of the files shall be transformed by typing the corresponding number.\")\n",
    "    else:\n",
    "        #Automatic iteration\n",
    "        print('Automatic Iteration.')\n",
    "        inp = index\n",
    "\n",
    "    input_file = file_list[int(inp)]\n",
    "\n",
    "    return (os.getcwd() + rel_path + input_file)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a help function to print petri nets\n",
    "def output_petri_net(net, initial_marking, final_marking, file_name, label):\n",
    "\n",
    "    #init visualizer\n",
    "    parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: OUTPUT_FORMAT, 'label':'The Round Table'}   #Add frequency to graph\n",
    "    gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters,\n",
    "                               variant=pn_visualizer.Variants.FREQUENCY, log=log)\n",
    "\n",
    "    gviz.attr(label=label)\n",
    "    pn_visualizer.save(gviz, os.getcwd() + REL_OUTPUT_PATH + file_name + \".\" + OUTPUT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(file_name,REL_OUTPUT_PATH = \"/Output Tree/\"):\n",
    "    return (os.getcwd() + REL_OUTPUT_PATH + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts a selected file in the path that is the input into a log\n",
    "def transform_to_log(file_path):\n",
    "    filename, file_extension = os.path.splitext(file_path)\n",
    "    x,z =os.path.split(file_path)\n",
    "    \n",
    "    if file_extension == '.csv':\n",
    "        log_csv = pd.read_csv(file,sep=None,encoding='utf-8-sig')\n",
    "        if z =='mobis_challenge_log_2019.csv' or z =='mobis_challenge_log_2019_only_complete_cases.csv':\n",
    "            log_csv['end'] = pd.to_datetime(log_csv['end'])\n",
    "            log_csv['start'] = pd.to_datetime(log_csv['start'])\n",
    "            log_csv['cost'] = log_csv['cost'].apply(pd.to_numeric, errors='coerce')\n",
    "            log_csv.rename(columns={'cost': 'case:cost','case':'case:concept:name','activity':'concept:name','end':'time:timestamp', 'user':'org:resource'}, inplace=True)\n",
    "        log_csv['time:timestamp'] = pd.to_datetime(log_csv['time:timestamp'])\n",
    "        log = log_converter.apply(log_csv)\n",
    "    elif file_extension == '.xes':\n",
    "        log = pm4py.read_xes(file_path)\n",
    "        log = pm4py.convert_to_event_log(log)\n",
    "    elif file_extension == '.dfg':\n",
    "        log = pm4py.read_dfg(file_path)\n",
    "    else:\n",
    "        print(\"Current filetype is equal to {}. \\nPlease input a file with any of the following extensions: - csv; - xes; - dfg\".format(str(file_extension)))\n",
    "        return -1\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_activities_from_log(log):\n",
    "    activities=[]\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            if activities.count(event['concept:name'])==0:\n",
    "                activities.append(event['concept:name'])\n",
    "    return activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function enriches each trace by the event 1...m, resource 1...m, Weekday start and end attributes until a given prefix length\n",
    "def complex_index_encoding(log, pref_length=5):\n",
    "    max_ev=0\n",
    "    for trace in log:\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "    \n",
    "    if pref_length > max_ev:\n",
    "        print('The prefix length is larger than the maximum trace length; Maximum trace length will be used.')\n",
    "        pref_length = max_ev\n",
    "\n",
    "    #weekdays\n",
    "    weekDaysMapping = (\"Monday\", \"Tuesday\",\n",
    "                    \"Wednesday\", \"Thursday\",\n",
    "                    \"Friday\", \"Saturday\",\n",
    "                    \"Sunday\")\n",
    "\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            trace.attributes['weekday_start']=weekDaysMapping[event['time:timestamp'].weekday()]\n",
    "            break\n",
    "    if pref_length == max_ev:\n",
    "        for trace in log:\n",
    "            for event in trace:\n",
    "                trace.attributes['weekday_end']=weekDaysMapping[event['time:timestamp'].weekday()]\n",
    "    \n",
    "    \n",
    "    j=0\n",
    "    no_evs={}\n",
    "    for trace in log:\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            if i==1:\n",
    "                st_time=event['time:timestamp'].day+event['time:timestamp'].hour/24+event['time:timestamp'].minute/(24*60)\n",
    "            if i<= pref_length:\n",
    "                trace.attributes['event_'+str(i)]=event['concept:name']\n",
    "                trace.attributes['resource_'+str(i)]=str(event['org:resource'])\n",
    "                trace.attributes['month_'+str(i)]=(str(event['time:timestamp'].month)+'_'+str(event['time:timestamp'].year))\n",
    "                #trace.attributes['elapsed_time']=event['time:timestamp'].day+event['time:timestamp'].hour/24+event['time:timestamp'].minute/(24*60)-st_time\n",
    "        no_evs[j]=i\n",
    "        j+=1\n",
    "\n",
    "    j=0\n",
    "    for trace in log:\n",
    "        if no_evs[j]<max_ev:\n",
    "            fill=no_evs[j]+1\n",
    "            for k in range(fill,max(max_ev,pref_length)+1):\n",
    "                trace.attributes['event_'+str(k)]=np.nan\n",
    "                trace.attributes['resource_'+str(k)]=np.nan\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffeine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "\"\"\"Settings\"\"\"\n",
    "##########\n",
    "# set the input and output path according to the files you want to select\n",
    "REL_INPUT_PATH = \"/../BPIC12/\" # here lie the event logs (.csv), the to-be model (.bpmn) and the already aligned traces (.pkl)\n",
    "REL_OUTPUT_PATH = \"/BPIC12/\"\n",
    "OUTPUT_FORMAT = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the log from the input path\n",
    "file= ask_for_path(REL_INPUT_PATH,14) # adjust to your path\n",
    "log=transform_to_log(file)\n",
    "ref_log=transform_to_log(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,21)# adjust to your path\n",
    "bpmn_graph = pm4py.read_bpmn(file)\n",
    "#pm4py.write_bpmn(bpmn_graph, \"ru.bpmn\", enable_layout=True)\n",
    "net, initial_marking, final_marking = bpmn_converter.apply(bpmn_graph)\n",
    "#net, initial_marking, final_marking=pm4py.read_pnml(file)\n",
    "# pm4py.visualization.petri_net.visualizer(net, initial_marking, final_marking)\n",
    "# output_petri_net(net, initial_marking, final_marking,'Basis_PN', 'test')\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_pkl(log, net, initial_marking, final_marking):\n",
    "    aligned_traces = pm4py.conformance_diagnostics_alignments(log, net, initial_marking, final_marking)\n",
    "    i=0\n",
    "    dev=[]\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i+=1\n",
    "\n",
    "    f = open('aligned_traces_mobis_complete.pkl','wb')\n",
    "    pickle.dump(aligned_traces,f)\n",
    "    f.close()\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,22)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    aligned_traces=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "## test data    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our FFN \n",
    "class BinaryClassificationIndiv(nn.Module):\n",
    "    def __init__(self, no_columns):\n",
    "        super(BinaryClassificationIndiv, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(no_columns, 256)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(256, 2)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(256)\n",
    "        self.batchnorm2 = nn.LayerNorm(256)\n",
    "        self.Softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LargerBinaryClassificationIndiv(nn.Module):\n",
    "    def __init__(self, no_columns):\n",
    "        super(LargerBinaryClassificationIndiv, self).__init__()\n",
    "        self.layer_1 = nn.Linear(no_columns, 512)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(512, 256)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_3 = nn.Linear(256, 256)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(256, 2)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(512)\n",
    "        self.batchnorm2 = nn.LayerNorm(256)\n",
    "        self.Softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation3(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "  def __init__(self, patience=15, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss <= self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for printing the accuracy during training - only informational\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.nn.functional.softmax(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def BPDP_classification_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False):\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i+=1\n",
    "\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000) # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace(r'^case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE=0.0001\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace(r'^case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "\n",
    "    path=(os.getcwd()+'/BPDP_Classifier') # output path\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path+'/'+z+'_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx]+1):\n",
    "                if y_cum_test[i][d][idx]==1: dev_position[d][idx]=i+1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness={}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training','Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test']=sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    dev_trained=[]\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] ==0:\n",
    "            metrics[d]='No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] ==0:\n",
    "            metrics[d]='No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "\n",
    "        Y_cum_dev={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            Y_cum_dev[prefix]=pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev']=0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i]=1-y_cum_test[prefix][d][i]\n",
    "            if prefix==1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat=ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind']=0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i]=i\n",
    "            imb_traces=pd.DataFrame(data=0, columns=['Dev'], index = range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace]>0:\n",
    "                    imb_traces['Dev'][trace]=1\n",
    "\n",
    "\n",
    "            imb_traces=imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat=imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "            imb_ref_enc_dat=pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat),columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss=OneSidedSelection(random_state=0,n_seeds_S=250,n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx= list(X_resampled['ind'])\n",
    "            y_train_idx= list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "        enumerated_trace_idx={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            drop_idx=[]\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx]< prefix:\n",
    "                    drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te=Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr=Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va=Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "            print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "            if prefix ==1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append( X_train, x_tr, axis=0)\n",
    "                X_test = np.append( X_test, x_te, axis=0)\n",
    "                y_train = np.append( y_train, y_tr, axis=0)\n",
    "                y_test = np.append( y_test, y_te, axis=0)\n",
    "                y_val = np.append( y_val, y_va, axis=0)\n",
    "                X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS=300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch<EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss=0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device))/len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "                    epoch_loss+=loss\n",
    "                    if i == len(steps)-1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model,vloss): done = True\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss: {epoch_loss/len(train_loader)}, Acc: {epoch_acc/len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss {epoch_loss/len(train_loader):}, Acc: {epoch_acc/len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision']=CM[0][1][1]/(CM[0][1][1]+CM[0][0][1])\n",
    "        metrics[d]['Recall']=CM[0][1][1]/(CM[0][1][1]+CM[0][1][0])\n",
    "        metrics[d]['Support']=CM[0][1][1]+CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] =  sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev'+d)]['Precision']=CM[1][1][1]/(CM[1][1][1]+CM[1][0][1])\n",
    "        metrics[str('NoDev'+d)]['Recall']=CM[1][1][1]/(CM[1][1][1]+CM[1][1][0])\n",
    "        metrics[str('NoDev'+d)]['Support']=CM[1][1][1]+CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        to_be_checked_idx={}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx=0\n",
    "            for prefix in range(1,event_count[idx]+1):\n",
    "                if prefix==1:\n",
    "                    to_be_checked_idx[idx]=[enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx)+cum_idx)\n",
    "                cum_idx+=len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "        for prefix in range(1, max_ev+1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx]>= prefix:\n",
    "                    if prefix==1:\n",
    "                        dev_position_pred[d][idx]=y_pred_list[to_be_checked_idx[idx][prefix-1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix-1]][0]==1 and y_pred_list[to_be_checked_idx[idx][prefix-2]][0]==0:\n",
    "                            if dev_position[d][idx]<= prefix:\n",
    "                                dev_position_pred[d][idx]=dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx]=prefix\n",
    "\n",
    "\n",
    "\n",
    "        earliness[d]=0\n",
    "        tobe_devs=0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx]==0 or dev_position_pred[d][idx]==0:\n",
    "                continue\n",
    "            tobe_devs+=1\n",
    "            earliness[d]+=dev_position_pred[d][idx]/dev_position[d][idx]\n",
    "        if not tobe_devs==0:\n",
    "            earliness[d]=earliness[d]/tobe_devs\n",
    "\n",
    "\n",
    "        if explained:\n",
    "            import shap\n",
    "            import matplotlib.pyplot as plt\n",
    "            np.random.seed(42)\n",
    "            e = shap.DeepExplainer(model, torch.FloatTensor(X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]))\n",
    "\n",
    "            shap_idx=[]\n",
    "            for j in range(len(y_pred_list)):\n",
    "                if y_pred_list[j][0]==y_test[j][0]==1:\n",
    "                    shap_idx.append(j)\n",
    "            shap_values = e.shap_values(torch.FloatTensor(X_test[shap_idx]))\n",
    "            fig=shap.summary_plot(shap_values[0], X_test[shap_idx], plot_type = 'dot', feature_names = enc_dat.columns, max_display=10, plot_size=(10,5), show=False)\n",
    "            plt.savefig(path+'/ShapValues/Dev_'+z+'_'+d+'.png')\n",
    "            plt.close()\n",
    "\n",
    "            fig=shap.summary_plot(shap_values[1], X_test[shap_idx], plot_type = 'dot', feature_names = enc_dat.columns, max_display=10, plot_size=(10,5), show=False)\n",
    "            plt.savefig(path+'/ShapValues/NoDev_'+z+'_'+d+'.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    avg_dev_pos={}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] ==0:\n",
    "            metrics[d]='No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] ==0:\n",
    "            metrics[d]='No Deviation in Training Set'\n",
    "            continue\n",
    "        devs=0\n",
    "        positions=0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx]>0:\n",
    "                devs+=1\n",
    "                positions+=dev_position[d][idx]\n",
    "        if devs ==0:\n",
    "            continue\n",
    "        avg_dev_pos[d]=positions/devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df=pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df=pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## load feature vectors from MPPN\n",
    "file = ask_for_path(REL_INPUT_PATH, 15)  # adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    pd_cases_fv = pickle.load(f)\n",
    "\n",
    "\n",
    "def BPDP_classification_MPPN(log, pd_cases_fv, aligned_traces, split=1 / 3, u_sample=True, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "    X_cum = {}\n",
    "    for pref in range(1, max_ev + 1):\n",
    "        X_cum[pref] = pd.DataFrame(columns=['FV'], index=list(range(len(pd_cases_fv['case:concept:name'].unique()))))\n",
    "    no_cases = len(pd_cases_fv['case:concept:name'].unique())\n",
    "    row = 0\n",
    "    for counter in range(len(pd_cases_fv)):\n",
    "        X_cum[len(pd_cases_fv.loc[counter].trace)]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "        if not counter == len(pd_cases_fv) - 1:\n",
    "            if not pd_cases_fv.loc[counter]['case:concept:name'] == pd_cases_fv.loc[counter + 1]['case:concept:name']:\n",
    "                if not len(pd_cases_fv.loc[counter].trace) == max_ev:\n",
    "                    for missing in range(len(pd_cases_fv.loc[counter].trace) + 1, max_ev + 1):\n",
    "                        X_cum[missing]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "                row += 1\n",
    "        else:\n",
    "            if not len(pd_cases_fv.loc[counter].trace) == max_ev:\n",
    "                for missing in range(len(pd_cases_fv.loc[counter].trace) + 1, max_ev + 1):\n",
    "                    X_cum[missing]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "            row += 1\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_MPPN_Classifier')\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_MPPN_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                        test_size=split, random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    dev_trained = []\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "        if dev_distribution[d]['Test'] / len(x_test_idx) <= 0.05:\n",
    "            metrics[d]['Notes'] = str(\n",
    "                'Only very few deviations in Test Set:' + str(dev_distribution[d]['Test'] / len(x_test_idx)))\n",
    "\n",
    "        elif dev_distribution[d]['Training'] / len(x_train_idx) <= 0.05:\n",
    "            metrics[d]['Notes'] = str(\n",
    "                'Only very few deviations in Training Set:' + str(dev_distribution[d]['Training'] / len(x_train_idx)))\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = pd.DataFrame(X_cum[1]['FV'].tolist()).add_prefix(\"c\")\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            P = pd.DataFrame(X_cum[prefix]['FV'].tolist()).add_prefix(\"c\")\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = P.loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = P.loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = P.loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "        LEARNING_RATE = 0.0001\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=128)\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx] >= prefix:\n",
    "                    if prefix == 1:\n",
    "                        dev_position_pred[d][idx] = y_pred_list[to_be_checked_idx[idx][prefix - 1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix - 1]][0] == 1 and \\\n",
    "                                y_pred_list[to_be_checked_idx[idx][prefix - 2]][0] == 0:\n",
    "                            if dev_position[d][idx] <= prefix:\n",
    "                                dev_position_pred[d][idx] = dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx] = prefix\n",
    "\n",
    "        earliness[d] = 0\n",
    "        tobe_devs = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] == 0 or dev_position_pred[d][idx] == 0:\n",
    "                continue\n",
    "            tobe_devs += 1\n",
    "            earliness[d] += dev_position_pred[d][idx] / dev_position[d][idx]\n",
    "        if not tobe_devs == 0:\n",
    "            earliness[d] = earliness[d] / tobe_devs\n",
    "\n",
    "    avg_dev_pos = {}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        devs = 0\n",
    "        positions = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] > 0:\n",
    "                devs += 1\n",
    "                positions += dev_position[d][idx]\n",
    "        if devs == 0:\n",
    "            continue\n",
    "        avg_dev_pos[d] = positions / devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df = pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df = pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def genga_benchmark(log, aligned_traces, c=2, alpha=1):\n",
    "    xt,z =os.path.split(file)\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    split = 1 / 3\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "    #if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "    for j, trace in enumerate(log):\n",
    "        for i, event in enumerate(trace):\n",
    "            pi = i + 1\n",
    "            if pi == 1:\n",
    "                event['duration'] = 0\n",
    "                event['trace'] = event['concept:name']\n",
    "\n",
    "\n",
    "            elif pi <= event_count[j]:\n",
    "                event['duration'] = (log[j][i]['time:timestamp'] - log[j][0]['time:timestamp']).total_seconds() / 60\n",
    "                event['trace'] = str(log[j][i - 1]['trace'] + ', ' + event['concept:name'])\n",
    "        trace.attributes['duration'] = log[j][event_count[j] - 1]['duration']\n",
    "    tree_df = pm4py.convert_to_dataframe(log)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_dataframe = tree_df.filter(items=['case:AMOUNT_REQ', 'duration'], axis=1)\n",
    "        ref_clean_dat = ref_dataframe.rename(columns={'case:AMOUNT_REQ': 'AMOUNT_REQ'})\n",
    "    else:\n",
    "        ref_clean_dat = tree_df.filter(items=['duration'], axis=1)\n",
    "\n",
    "    X_tree = ref_clean_dat.loc[x_train_idx]\n",
    "    for d in dev:\n",
    "        y_tree = dev_df[d][x_train_idx]\n",
    "\n",
    "        from feature_engine.discretisation import DecisionTreeDiscretiser\n",
    "\n",
    "        disc = DecisionTreeDiscretiser(regression=False)\n",
    "\n",
    "        # fit the transformer\n",
    "        disc.fit(X_tree, y_tree)\n",
    "\n",
    "        tree_df = pm4py.convert_to_dataframe(log)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            X_preprosessed = tree_df.filter(items=['case:AMOUNT_REQ', 'duration'], axis=1)\n",
    "            X_preprosessed = X_preprosessed.rename(columns={'case:AMOUNT_REQ': 'AMOUNT_REQ'})\n",
    "        else:\n",
    "            X_preprosessed = tree_df.filter(items=['duration'], axis=1)\n",
    "\n",
    "        try:\n",
    "            X_preprosessed = disc.transform(X_preprosessed)\n",
    "        except Exception as er:\n",
    "            print(er)\n",
    "\n",
    "\n",
    "        genga_states = []\n",
    "        runner = 0\n",
    "        for j, trace in enumerate(log):\n",
    "            for i, event in enumerate(trace):\n",
    "                if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "                    event['genga'] = str(\n",
    "                        event['trace'] + '_' + str(X_preprosessed['AMOUNT_REQ'][runner + i]) + '_' + str(\n",
    "                            X_preprosessed['duration'][runner + i]))\n",
    "                    if not event['genga'] in genga_states:\n",
    "                        genga_states.append(event['genga'])\n",
    "                else:\n",
    "                    event['genga'] = str(\n",
    "                        event['trace'] + '_' + str(\n",
    "                            X_preprosessed['duration'][runner + i]))\n",
    "                    if not event['genga'] in genga_states:\n",
    "                        genga_states.append(event['genga'])\n",
    "            runner += event_count[j]\n",
    "\n",
    "        len(genga_states)\n",
    "        genga_counts = pd.DataFrame(data=0, index=genga_states, columns=[d])\n",
    "        genga_counts['count'] = 0\n",
    "        for j, trace in enumerate(log):\n",
    "            if not j in x_train_idx:\n",
    "                continue\n",
    "            for i, event in enumerate(trace):\n",
    "                genga_counts[d][event['genga']] += dev_df[d][j]\n",
    "                genga_counts['count'][event['genga']] += 1\n",
    "        genga_counts['count'].sum()\n",
    "        test_count = 0\n",
    "        for idx in y_test_idx:\n",
    "            test_count += event_count[idx]\n",
    "        y_true_df = pd.DataFrame(data=0, index=range(test_count), columns=[d])\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max(event_count.values()) + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "\n",
    "        runner = 0\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                y_true_df[d][runner] = y_cum_test[prefix][d][enumerated_trace_idx[prefix][idx]]\n",
    "                runner += 1\n",
    "\n",
    "        genga_states_test = {}\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            genga_states_test[prefix] = []\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                genga_states_test[prefix].append(log[enumerated_trace_idx[prefix][idx]][prefix - 1]['genga'])\n",
    "\n",
    "        y_pred_df = pd.DataFrame(data=0, index=range(test_count), columns=[d])\n",
    "\n",
    "        runner = 0\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                p = genga_counts[d][genga_states_test[prefix][idx]]\n",
    "                n = genga_counts['count'][genga_states_test[prefix][idx]] - genga_counts[d][genga_states_test[prefix][idx]]\n",
    "                xb = p / (p + n + c)\n",
    "                xd = n / (p + n + c)\n",
    "                xu = c / (p + n + c)\n",
    "                if (xu < xb or xu < xd) and xb > alpha * xd:\n",
    "                    y_pred_df[d][runner] = 1\n",
    "                runner += 1\n",
    "        y_test_list = y_true_df.values.tolist()\n",
    "        y_pred_list = y_pred_df.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        CM = sklearn.metrics.confusion_matrix(y_test_list, y_pred_list)\n",
    "\n",
    "\n",
    "        try:\n",
    "            metrics[d]['Precision'] = CM[1][1] / (CM[1][1] + CM[0][1])\n",
    "            metrics[d]['Recall'] = CM[1][1] / (CM[1][1] + CM[1][0])\n",
    "        except Exception as er:\n",
    "             metrics[d]['Precision']=CM[0]\n",
    "             metrics[d]['Recall']=er\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_true_df[d], y_pred_df[d], average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        try:\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[0][0] / (CM[0][0] + CM[1][0])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[0][0] / (CM[0][0] + CM[0][1])\n",
    "        except Exception as er:\n",
    "            metrics[str('NoDev' + d)]['ROC_AUC'] = er\n",
    "\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "    path = (os.getcwd() + '/Genga')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_Genga_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classify_cat(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    # print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace(r'^case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace(r'^case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        cat_y_train = y_train[:, 0]\n",
    "        cat_y_val = y_val[:, 0]\n",
    "        cat_y_test = y_test[:, 0]\n",
    "        cat_y_train\n",
    "        from catboost import CatBoostClassifier\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=16, early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(X_train, cat_y_train)\n",
    "            y_pred = catboost.predict(X_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "        except Exception as er:\n",
    "            metrics[d]=er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classify_xgb(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    # print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace(r'^case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace(r'^case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        cxgb_y_train = y_train[:, 0]\n",
    "        xgb_y_val = y_val[:, 0]\n",
    "        xgb_y_test = y_test[:, 0]\n",
    "        xgb_y_train\n",
    "        bst = xgb.XGBClassifier(max_depth=16, scale_pos_weight=16)\n",
    "\n",
    "        try:\n",
    "            bst.fit(X_train, xgb_y_train, eval_set=[(X_train, xgb_y_train), (X_val, xgb_y_val)], early_stopping_rounds=10)\n",
    "            y_preds = bst.predict(X_test)\n",
    "            y_preds\n",
    "            y_preds = np.stack((y_preds, 1 - y_preds), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "        except Exception as er:\n",
    "            metrics[d]=er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Suffix Prediction: Load Suffixes and execute prediction of deviations\n",
    "file= ask_for_path(REL_INPUT_PATH,25)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    suffixes=pickle.load(f)\n",
    "def suffix_prediction_deviations(log, aligned_traces, net, initial_marking, final_marking, suffixes,split = 1 / 3):\n",
    "    xt, z = os.path.split(file)\n",
    "    if z == 'MPPN_BPIC_2020_request_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_international_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_prepaid_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_domestic_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    suffixes = suffixes.rename(columns={'case:concept:name': 'case:trace_ID', 'IDX': 'case:IDX'})\n",
    "    suffixes['case:concept:name'] = suffixes[[\"case:trace_ID\", \"case:IDX\"]].astype(str).apply(\"_\".join, axis=1)\n",
    "\n",
    "    suffixes['case:prefix'] = 1\n",
    "    for suf in range(1, len(suffixes)):\n",
    "        if suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and suffixes['case:IDX'][suf - 1] == \\\n",
    "                suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1]\n",
    "        elif suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and not suffixes['case:IDX'][suf - 1] == \\\n",
    "                                                                                          suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1] + 1\n",
    "\n",
    "    events_per_trace = pd.DataFrame(data=0, columns=['count'], index=suffixes['case:IDX'].unique())\n",
    "    for idx in range(len(suffixes)):\n",
    "        events_per_trace['count'][suffixes['case:IDX'][idx]] += 1\n",
    "\n",
    "    predicted_log = pm4py.convert_to_event_log(suffixes)\n",
    "\n",
    "\n",
    "    aligned_predictions = pm4py.conformance_diagnostics_alignments(predicted_log, net, initial_marking, final_marking)\n",
    "\n",
    "    i = 0\n",
    "    pred_dev = []\n",
    "    for trace in predicted_log:\n",
    "        no_moves = len(aligned_predictions[i]['alignment'])\n",
    "        for j in range(0, len(aligned_predictions[i]['alignment'])):\n",
    "            if aligned_predictions[i]['alignment'][j][1] == None or aligned_predictions[i]['alignment'][j][0] == \\\n",
    "                    aligned_predictions[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_predictions[i]['alignment'][j]) in pred_dev:\n",
    "                    pred_dev.append(str(aligned_predictions[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    dev_df['trace_ID'] = 0\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        dev_df['trace_ID'][k] = trace.attributes['concept:name']\n",
    "        k += 1\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    caseIDs_test = []\n",
    "    key_caseID_testIDX = {}\n",
    "    for idx in x_test_idx:\n",
    "        caseIDs_test.append(log[idx].attributes['concept:name'])\n",
    "        key_caseID_testIDX[log[idx].attributes['concept:name']] = idx\n",
    "    caseIDs_train = []\n",
    "    for idx in x_train_idx:\n",
    "        caseIDs_train.append(log[idx].attributes['concept:name'])\n",
    "    y_cum_pred = {}\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_pred[\n",
    "            ev] = pd.DataFrame(data=0, columns=dev, index=caseIDs_test)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].set_index('trace_ID')\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "    for i, alignment in enumerate(aligned_predictions):\n",
    "        no_moves = len(alignment['alignment'])\n",
    "        for d in dev:\n",
    "            for j in range(no_moves):\n",
    "                if str(aligned_predictions[i]['alignment'][j]) == d:\n",
    "                    #print(i, predicted_log[i].attributes['trace_ID'],d, predicted_log[i].attributes['prefix'])\n",
    "                    y_cum_pred[predicted_log[i].attributes['prefix']][d][predicted_log[i].attributes['trace_ID']] = 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[ev] = y_cum_test[ev].sort_index()\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].sort_index()\n",
    "\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred[ev]\n",
    "            y_test = y_cum_test[ev]\n",
    "        else:\n",
    "\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test[ev], axis=0)\n",
    "    len(y_cum_test[1].values.tolist())\n",
    "    print(len(y_pred_list))\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                       average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "    path = (os.getcwd() + '/Suffix_Prediction')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_suffix_pred_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Here start the comparisons to other design choices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def BPDP_no_imbalance(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt,z =os.path.split(file)\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i+=1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace(r'^case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    min_pref=1\n",
    "    max_pref=max_ev\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE=0.0001\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace(r'^case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    import math\n",
    "\n",
    "\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "\n",
    "    path=(os.getcwd()+'/BPDP_Classifier')\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path+'/'+z+'_MC_CrossEntropyLoss_NoImbalance_'+str(early_stop)+'_'+str(round(split,2))+'.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "\n",
    "        Y_cum_dev={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            Y_cum_dev[prefix]=pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev']=0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i]=1-y_cum_test[prefix][d][i]\n",
    "            if prefix==1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "\n",
    "        x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "        print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "        enumerated_trace_idx={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            drop_idx=[]\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx]< prefix:\n",
    "                    drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te=Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr=Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va=Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "            print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "            if prefix ==1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append( X_train, x_tr, axis=0)\n",
    "                X_test = np.append( X_test, x_te, axis=0)\n",
    "                y_train = np.append( y_train, y_tr, axis=0)\n",
    "                y_test = np.append( y_test, y_te, axis=0)\n",
    "                y_val = np.append( y_val, y_va, axis=0)\n",
    "                X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if not early_stop:\n",
    "\n",
    "            print(dev, 'training start no ES')\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            for e in range(1, EPOCHS+1):\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    y_pred = model(X_batch)\n",
    "\n",
    "                    loss = criterion(y_pred.unsqueeze(1), y_batch.unsqueeze(1))\n",
    "                    acc = binary_acc(y_pred.unsqueeze(1), y_batch.unsqueeze(1))/len(y_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "                print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} |', d)\n",
    "\n",
    "        else:\n",
    "            print('training start with ES')\n",
    "            EPOCHS=300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch<EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss=0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device))/len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "                    epoch_loss+=loss\n",
    "                    if i == len(steps)-1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model,vloss): done = True\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss: {epoch_loss/len(train_loader)}, Acc: {epoch_acc/len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss {epoch_loss/len(train_loader):}, Acc: {epoch_acc/len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision']=CM[0][1][1]/(CM[0][1][1]+CM[0][0][1])\n",
    "        metrics[d]['Recall']=CM[0][1][1]/(CM[0][1][1]+CM[0][1][0])\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] =  sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev'+d)]['Precision']=CM[1][1][1]/(CM[1][1][1]+CM[1][0][1])\n",
    "        metrics[str('NoDev'+d)]['Recall']=CM[1][1][1]/(CM[1][1][1]+CM[1][1][0])\n",
    "        print(CM)\n",
    "\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we define our FFN\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, no_columns, no_devs):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(no_columns, 2048)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(2048, 2048)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_3 = nn.Linear(2048, 1024)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(1024, no_devs)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(2048)\n",
    "        self.batchnorm2 = nn.LayerNorm(1024)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation3(self.layer_3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def BPDP_single_classifier(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True):\n",
    "    path=(os.getcwd()+'/Evaluation')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace(r'^case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    from imblearn.under_sampling import OneSidedSelection\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "    else:\n",
    "        x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                            test_size=split, random_state=0)\n",
    "\n",
    "    min_pref = 1\n",
    "    max_pref = max_ev\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    N = len(dev_df)\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (4 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "\n",
    "\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                      random_state=0)\n",
    "    X_cum = {}\n",
    "    enumerated_trace_idx = {}\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace(r'^case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "        enc_dat = enc_dat.drop(drop_idx)\n",
    "        x_te = enc_dat.loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_tr = enc_dat.loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_va = enc_dat.loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "\n",
    "        print(prefix)\n",
    "        if prefix == 1:\n",
    "            X_train = x_tr\n",
    "            X_test = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val = x_va\n",
    "            y_val = y_va\n",
    "        else:\n",
    "            X_train = np.append(X_train, x_tr, axis=0)\n",
    "            X_test = np.append(X_test, x_te, axis=0)\n",
    "            y_train = np.append(y_train, y_tr, axis=0)\n",
    "            y_test = np.append(y_test, y_te, axis=0)\n",
    "            y_val = np.append(y_val, y_va, axis=0)\n",
    "            X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(len(X_train), len(y_train), len(X_test), len(y_test))\n",
    "    print('split done')\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BinaryClassification(no_columns=len(ref_enc_dat.loc[0]), no_devs=len(dev))\n",
    "    model.to(device)\n",
    "    criterion = nn.MultiLabelSoftMarginLoss(weight=torch.FloatTensor(list(positive_weights.values())))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if not early_stop:\n",
    "\n",
    "        print('training start no ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        for e in range(1, EPOCHS + 1):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                acc = binary_acc(y_pred, y_batch) / len(y_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "            print(\n",
    "                f'Epoch {e + 0:03}: | Loss: {epoch_loss / len(train_loader):.5f} | Acc: {epoch_acc / len(train_loader):.3f} |',\n",
    "                prefix)\n",
    "\n",
    "    else:\n",
    "        print('training start with ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                #print(y_batch_pred.shape)\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                current = (i + 1) * len(x_batch)\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(X_val)\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_data = TestData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(torch.sigmoid_(y_test_pred))\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:,i], np.array(y_pred_list)[:,i], average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_BPDP_single_classifier_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.save()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BPDP_single_classifier(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
