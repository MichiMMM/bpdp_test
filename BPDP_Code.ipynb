{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "98590ff4fe04c8543246b2a01debd3de3c5ca9b666f43f1fa87d5110c692004c"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# We import the neccessary packages in the beginning\n",
    "import os\n",
    "import math\n",
    "from statistics import mean,stdev\n",
    "import pm4py\n",
    "from pm4py.objects.conversion.log import converter as log_converter\n",
    "from pm4py.objects.conversion.bpmn import converter as bpmn_converter\n",
    "from sklearn.impute import SimpleImputer\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from imblearn.under_sampling import OneSidedSelection\n",
    "from sklearn.preprocessing import StandardScaler    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import sklearn\n",
    "import tqdm\n",
    "import time\n",
    "import xgboost as xgb"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "# Returns a path to the file selected by the user\n",
    "# Input: The folder in which to look for the files - the default is the current folder\n",
    "def ask_for_path(rel_path='', index = -1):\n",
    "    #Crawl all files in the input folder\n",
    "    print(\"The following files are available in the input folder:\\n\")\n",
    "\n",
    "    count = 0\n",
    "    file_list = os.listdir(os.getcwd() + rel_path)\n",
    "    for file in file_list:\n",
    "        print(str(count) + \" - \" + file)\n",
    "        count+=1\n",
    "\n",
    "    if(index == -1):\n",
    "        #Ask for which of the files shall be transformed and select it.\n",
    "        inp = input(\"Please choose from the list above which of the files shall be transformed by typing the corresponding number.\")\n",
    "    else:\n",
    "        #Automatic iteration\n",
    "        print('Automatic Iteration.')\n",
    "        inp = index\n",
    "\n",
    "    input_file = file_list[int(inp)]\n",
    "\n",
    "    return (os.getcwd() + rel_path + input_file)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a help function to print petri nets\n",
    "def output_petri_net(net, initial_marking, final_marking, file_name, label):\n",
    "\n",
    "    #init visualizer\n",
    "    parameters = {pn_visualizer.Variants.FREQUENCY.value.Parameters.FORMAT: OUTPUT_FORMAT, 'label':'The Round Table'}   #Add frequency to graph\n",
    "    gviz = pn_visualizer.apply(net, initial_marking, final_marking, parameters=parameters,\n",
    "                               variant=pn_visualizer.Variants.FREQUENCY, log=log)\n",
    "\n",
    "    gviz.attr(label=label)\n",
    "    pn_visualizer.save(gviz, os.getcwd() + REL_OUTPUT_PATH + file_name + \".\" + OUTPUT_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_path(file_name,REL_OUTPUT_PATH = \"/Output Tree/\"):\n",
    "    return (os.getcwd() + REL_OUTPUT_PATH + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function converts a selected file in the path that is the input into a log\n",
    "def transform_to_log(file_path):\n",
    "    filename, file_extension = os.path.splitext(file_path)\n",
    "    x,z =os.path.split(file_path)\n",
    "    \n",
    "    if file_extension == '.csv':\n",
    "        log_csv = pd.read_csv(file,sep=None,encoding='utf-8-sig')\n",
    "        if z =='mobis_challenge_log_2019.csv' or z =='mobis_challenge_log_2019_only_complete_cases.csv':\n",
    "            log_csv['end'] = pd.to_datetime(log_csv['end'])\n",
    "            log_csv['start'] = pd.to_datetime(log_csv['start'])\n",
    "            log_csv['cost'] = log_csv['cost'].apply(pd.to_numeric, errors='coerce')\n",
    "            log_csv.rename(columns={'cost': 'case:cost','case':'case:concept:name','activity':'concept:name','end':'time:timestamp', 'user':'org:resource'}, inplace=True)\n",
    "        elif z =='mobis_challenge_log_2019_original.csv':\n",
    "            log_csv['end'] = pd.to_datetime(log_csv['end'])\n",
    "            log_csv['start'] = pd.to_datetime(log_csv['start'])\n",
    "            log_csv['cost'] = log_csv['cost'].apply(pd.to_numeric, errors='coerce')\n",
    "            log_csv.rename(columns={'case':'case:concept:name','activity':'concept:name','start':'time:timestamp', 'user':'org:resource'}, inplace=True)\n",
    "        log_csv['time:timestamp'] = pd.to_datetime(log_csv['time:timestamp'])\n",
    "        log = log_converter.apply(log_csv)\n",
    "\n",
    "    elif file_extension == '.xes':\n",
    "        log = pm4py.read_xes(file_path)\n",
    "        log = pm4py.convert_to_event_log(log)\n",
    "    elif file_extension == '.dfg':\n",
    "        log = pm4py.read_dfg(file_path)\n",
    "    else:\n",
    "        print(\"Current filetype is equal to {}. \\nPlease input a file with any of the following extensions: - csv; - xes; - dfg\".format(str(file_extension)))\n",
    "        return -1\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_activities_from_log(log):\n",
    "    activities=[]\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            if activities.count(event['concept:name'])==0:\n",
    "                activities.append(event['concept:name'])\n",
    "    return activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function enriches each trace by the event 1...m, resource 1...m, Weekday start and end attributes until a given prefix length\n",
    "def complex_index_encoding(log, pref_length=5):\n",
    "    max_ev=0\n",
    "    for trace in log:\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "    \n",
    "    if pref_length > max_ev:\n",
    "        print('The prefix length is larger than the maximum trace length; Maximum trace length will be used.')\n",
    "        pref_length = max_ev\n",
    "\n",
    "    #weekdays\n",
    "    weekDaysMapping = (\"Monday\", \"Tuesday\",\n",
    "                    \"Wednesday\", \"Thursday\",\n",
    "                    \"Friday\", \"Saturday\",\n",
    "                    \"Sunday\")\n",
    "\n",
    "    for trace in log:\n",
    "        for event in trace:\n",
    "            trace.attributes['weekday_start']=weekDaysMapping[event['time:timestamp'].weekday()]\n",
    "            break\n",
    "    if pref_length == max_ev:\n",
    "        for trace in log:\n",
    "            for event in trace:\n",
    "                trace.attributes['weekday_end']=weekDaysMapping[event['time:timestamp'].weekday()]\n",
    "    \n",
    "    \n",
    "    j=0\n",
    "    no_evs={}\n",
    "    for trace in log:\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            if i==1:\n",
    "                st_time=event['time:timestamp'].day+event['time:timestamp'].hour/24+event['time:timestamp'].minute/(24*60)\n",
    "            if i<= pref_length:\n",
    "                trace.attributes['event_'+str(i)]=event['concept:name']\n",
    "                trace.attributes['resource_'+str(i)]=str(event['org:resource'])\n",
    "                trace.attributes['month_'+str(i)]=(str(event['time:timestamp'].month)+'_'+str(event['time:timestamp'].year))\n",
    "                #trace.attributes['elapsed_time']=event['time:timestamp'].day+event['time:timestamp'].hour/24+event['time:timestamp'].minute/(24*60)-st_time\n",
    "        no_evs[j]=i\n",
    "        j+=1\n",
    "\n",
    "    j=0\n",
    "    for trace in log:\n",
    "        if no_evs[j]<max_ev:\n",
    "            fill=no_evs[j]+1\n",
    "            for k in range(fill,max(max_ev,pref_length)+1):\n",
    "                trace.attributes['event_'+str(k)]=np.nan\n",
    "                trace.attributes['resource_'+str(k)]=np.nan\n",
    "\n",
    "    return log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import caffeine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "\"\"\"Settings\"\"\"\n",
    "##########\n",
    "# set the input and output path according to the files you want to select\n",
    "REL_INPUT_PATH = \"/../BPIC12/\" # here lie the event logs (.csv), the to-be model (.bpmn) and the already aligned traces (.pkl)\n",
    "REL_OUTPUT_PATH = \"/../BPIC12/\"\n",
    "OUTPUT_FORMAT = \"png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the log from the input path\n",
    "file= ask_for_path(REL_INPUT_PATH,9) # adjust to your path\n",
    "log=transform_to_log(file)\n",
    "ref_log=transform_to_log(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,0)# adjust to your path\n",
    "bpmn_graph = pm4py.read_bpmn(file)\n",
    "#pm4py.write_bpmn(bpmn_graph, \"ru.bpmn\", enable_layout=True)\n",
    "net, initial_marking, final_marking = bpmn_converter.apply(bpmn_graph)\n",
    "#net, initial_marking, final_marking=pm4py.read_pnml(file)\n",
    "# pm4py.visualization.petri_net.visualizer(net, initial_marking, final_marking)\n",
    "# output_petri_net(net, initial_marking, final_marking,'Basis_PN', 'test')\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_alignments_pkl(log, net, initial_marking, final_marking):\n",
    "    aligned_traces = pm4py.conformance_diagnostics_alignments(log, net, initial_marking, final_marking)\n",
    "    i=0\n",
    "    dev=[]\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i+=1\n",
    "\n",
    "    f = open('aligned_traces_binet_12A.pkl','wb')\n",
    "    pickle.dump(aligned_traces,f)\n",
    "    f.close()\n",
    "    return dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#dev, aligned_traces=generate_alignments_pkl(log, net, initial_marking, final_marking)\n",
    "#print(len(dev))\n",
    "#dev"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,13)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    aligned_traces=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train data\n",
    "class TrainData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index], self.y_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)\n",
    "\n",
    "## test data    \n",
    "class TestData(Dataset):\n",
    "    \n",
    "    def __init__(self, X_data):\n",
    "        self.X_data = X_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X_data[index]\n",
    "        \n",
    "    def __len__ (self):\n",
    "        return len(self.X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our FFN \n",
    "class BinaryClassificationIndiv(nn.Module):\n",
    "    def __init__(self, no_columns):\n",
    "        super(BinaryClassificationIndiv, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(no_columns, 256)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(256, 256)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(256, 2)\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(256)\n",
    "        self.batchnorm2 = nn.LayerNorm(256)\n",
    "        self.Softmax = nn.Softmax()\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class BPDP_LSTM(nn.Module):\n",
    "    def __init__(self, vocab_events, vocab_resources, no_TA, vocab_month):\n",
    "        super(BPDP_LSTM, self).__init__()\n",
    "        self.embedding_e = nn.Embedding(vocab_events, 16) # hier auf 8 / 16\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.lstm_e = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "        self.linear_e = nn.Linear(64, 32)\n",
    "        self.embedding_r = nn.Embedding(vocab_resources, 16)\n",
    "        self.lstm_r = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "        self.linear_r = nn.Linear(64, 32)\n",
    "        self.embedding_m = nn.Embedding(vocab_month, 16)\n",
    "        self.lstm_m = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "        self.linear_m = nn.Linear(64, 32)\n",
    "        self.linear_ta = nn.Linear(no_TA, 32)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(128)\n",
    "        self.linear = nn.Linear(128, 2)\n",
    "    def forward(self, evs, rs,tas, ms):\n",
    "        evs= self.embedding_e(evs)\n",
    "        evs, _ = self.lstm_e(evs)\n",
    "        evs=self.linear_e(evs)\n",
    "        evs=evs[:, -1, :]\n",
    "        evs=self.activation1(evs)\n",
    "        rs= self.embedding_r(rs)\n",
    "        rs, _ = self.lstm_r(rs)\n",
    "        rs=rs[:, -1, :]\n",
    "        rs=self.activation1(rs)\n",
    "        rs=self.linear_r(rs)\n",
    "        ms= self.embedding_m(ms)\n",
    "        ms, _ = self.lstm_m(ms)\n",
    "        ms=ms[:, -1, :]\n",
    "        ms=self.activation1(ms)\n",
    "        ms=self.linear_m(ms)\n",
    "        tas= self.linear_ta(tas)\n",
    "        fin=torch.cat((evs,rs),dim=1)\n",
    "        fin=torch.cat((fin,ms),dim=1)\n",
    "        fin=torch.cat((fin,tas),dim=1)\n",
    "        fin=self.batchnorm1(fin)\n",
    "        #fin = self.dropout(fin)\n",
    "        fin = self.linear(fin)\n",
    "        return fin"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class LargerBinaryClassificationIndiv(nn.Module):\n",
    "    def __init__(self, no_columns):\n",
    "        super(LargerBinaryClassificationIndiv, self).__init__()\n",
    "        self.layer_1 = nn.Linear(no_columns, 512)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(512, 256)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_3 = nn.Linear(256, 256)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(256, 2)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(512)\n",
    "        self.batchnorm2 = nn.LayerNorm(256)\n",
    "        self.Softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.activation3(self.layer_3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping():\n",
    "  def __init__(self, patience=10, min_delta=0, restore_best_weights=True):\n",
    "    self.patience = patience\n",
    "    self.min_delta = min_delta\n",
    "    self.restore_best_weights = restore_best_weights\n",
    "    self.best_model = None\n",
    "    self.best_loss = None\n",
    "    self.counter = 0\n",
    "    self.status = \"\"\n",
    "    \n",
    "  def __call__(self, model, val_loss):\n",
    "    if self.best_loss == None:\n",
    "      self.best_loss = val_loss\n",
    "      self.best_model = copy.deepcopy(model)\n",
    "    elif self.best_loss - val_loss > self.min_delta:\n",
    "      self.best_loss = val_loss\n",
    "      self.counter = 0\n",
    "      self.best_model.load_state_dict(model.state_dict())\n",
    "    elif self.best_loss - val_loss <= self.min_delta:\n",
    "      self.counter += 1\n",
    "      if self.counter >= self.patience:\n",
    "        self.status = f\"Stopped on {self.counter}\"\n",
    "        if self.restore_best_weights:\n",
    "          model.load_state_dict(self.best_model.state_dict())\n",
    "        return True\n",
    "    self.status = f\"{self.counter}/{self.patience}\"\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just for printing the accuracy during training - only informational\n",
    "def binary_acc(y_pred, y_test):\n",
    "    y_pred_tag = torch.round(torch.nn.functional.softmax(y_pred))\n",
    "\n",
    "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
    "    acc = correct_results_sum/y_test.shape[0]\n",
    "    acc = torch.round(acc * 100)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i+=1\n",
    "\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000) # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE=0.0001\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','LenTrain', 'LenTrain_beforeUS_0', 'LenTrain_beforeUS_1', 'LenTrain_afterUS_0', 'LenTrain_afterUS_1'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "\n",
    "\n",
    "    path=(os.getcwd()+'/BPDP_Classifier') # output path\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path+'/'+z+'_BPDP_CIBE_classification_testcount.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "        metrics[d]['LenTrain_beforeUS_1']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        metrics[d]['LenTrain_beforeUS_0']=len(x_train_idx)-sum(dev_df[d][i] for i in x_train_idx)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx]+1):\n",
    "                if y_cum_test[i][d][idx]==1: dev_position[d][idx]=i+1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness={}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training','Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test']=sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    dev_trained=[]\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] ==0:\n",
    "            metrics[d]='No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] ==0:\n",
    "            metrics[d]='No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "\n",
    "        Y_cum_dev={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            Y_cum_dev[prefix]=pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev']=0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i]=1-y_cum_test[prefix][d][i]\n",
    "            if prefix==1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat=ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind']=0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i]=i\n",
    "            imb_traces=pd.DataFrame(data=0, columns=['Dev'], index = range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace]>0:\n",
    "                    imb_traces['Dev'][trace]=1\n",
    "\n",
    "\n",
    "            imb_traces=imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat=imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "            imb_ref_enc_dat=pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat),columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss=OneSidedSelection(random_state=0,n_seeds_S=250,n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx= list(X_resampled['ind'])\n",
    "            y_train_idx= list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "        metrics[d]['LenTrain']=len(x_train_idx)\n",
    "        metrics[d]['LenTrain_afterUS_1']=imb_traces['Dev'].sum()\n",
    "        metrics[d]['LenTrain_afterUS_0']=len(x_train_idx)-imb_traces['Dev'].sum()\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "        enumerated_trace_idx={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            drop_idx=[]\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx]< prefix:\n",
    "                    drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te=Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr=Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va=Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "            print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "            if prefix ==1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append( X_train, x_tr, axis=0)\n",
    "                X_test = np.append( X_test, x_te, axis=0)\n",
    "                y_train = np.append( y_train, y_tr, axis=0)\n",
    "                y_test = np.append( y_test, y_te, axis=0)\n",
    "                y_val = np.append( y_val, y_va, axis=0)\n",
    "                X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS=300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch<EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss=0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device))/len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "                    epoch_loss+=loss\n",
    "                    if i == len(steps)-1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model,vloss): done = True\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss: {epoch_loss/len(train_loader)}, Acc: {epoch_acc/len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss {epoch_loss/len(train_loader):}, Acc: {epoch_acc/len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision']=CM[0][1][1]/(CM[0][1][1]+CM[0][0][1])\n",
    "        metrics[d]['Recall']=CM[0][1][1]/(CM[0][1][1]+CM[0][1][0])\n",
    "        metrics[d]['Support']=CM[0][1][1]+CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] =  sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev'+d)]['Precision']=CM[1][1][1]/(CM[1][1][1]+CM[1][0][1])\n",
    "        metrics[str('NoDev'+d)]['Recall']=CM[1][1][1]/(CM[1][1][1]+CM[1][1][0])\n",
    "        metrics[str('NoDev'+d)]['Support']=CM[1][1][1]+CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        to_be_checked_idx={}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx=0\n",
    "            for prefix in range(1,event_count[idx]+1):\n",
    "                if prefix==1:\n",
    "                    to_be_checked_idx[idx]=[enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx)+cum_idx)\n",
    "                cum_idx+=len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "        for prefix in range(1, max_ev+1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx]>= prefix:\n",
    "                    if prefix==1:\n",
    "                        dev_position_pred[d][idx]=y_pred_list[to_be_checked_idx[idx][prefix-1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix-1]][0]==1 and y_pred_list[to_be_checked_idx[idx][prefix-2]][0]==0:\n",
    "                            if dev_position[d][idx]<= prefix:\n",
    "                                dev_position_pred[d][idx]=dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx]=prefix\n",
    "\n",
    "\n",
    "\n",
    "        earliness[d]=0\n",
    "        tobe_devs=0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx]==0 or dev_position_pred[d][idx]==0:\n",
    "                continue\n",
    "            tobe_devs+=1\n",
    "            earliness[d]+=dev_position_pred[d][idx]/dev_position[d][idx]\n",
    "        if not tobe_devs==0:\n",
    "            earliness[d]=earliness[d]/tobe_devs\n",
    "\n",
    "\n",
    "        if explained:\n",
    "            import shap\n",
    "            import matplotlib.pyplot as plt\n",
    "            np.random.seed(42)\n",
    "            e = shap.DeepExplainer(model, torch.FloatTensor(X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]))\n",
    "\n",
    "            shap_idx=[]\n",
    "            for j in range(len(y_pred_list)):\n",
    "                if y_pred_list[j][0]==y_test[j][0]==1:\n",
    "                    shap_idx.append(j)\n",
    "            shap_values = e.shap_values(torch.FloatTensor(X_test[shap_idx]))\n",
    "            fig=shap.summary_plot(shap_values[0], X_test[shap_idx], plot_type = 'dot', feature_names = enc_dat.columns, max_display=10, plot_size=(10,5), show=False)\n",
    "            plt.savefig(path+'/ShapValues/Dev_'+z+'_'+d+'.png')\n",
    "            plt.close()\n",
    "\n",
    "            fig=shap.summary_plot(shap_values[1], X_test[shap_idx], plot_type = 'dot', feature_names = enc_dat.columns, max_display=10, plot_size=(10,5), show=False)\n",
    "            plt.savefig(path+'/ShapValues/NoDev_'+z+'_'+d+'.png')\n",
    "            plt.close()\n",
    "\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    avg_dev_pos={}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] ==0:\n",
    "            metrics[d]='No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] ==0:\n",
    "            metrics[d]='No Deviation in Training Set'\n",
    "            continue\n",
    "        devs=0\n",
    "        positions=0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx]>0:\n",
    "                devs+=1\n",
    "                positions+=dev_position[d][idx]\n",
    "        if devs ==0:\n",
    "            continue\n",
    "        avg_dev_pos[d]=positions/devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df=pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df=pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_LSTM_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "\n",
    "    ref_enc_dat = ref_clean_dat.copy()\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','Time'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = clean_dat.copy()\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 'No'  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No')\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_LSTM')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter('BPDP_LSTM' + '/' + z + '_BPDP_LSTM_time_stopped.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    ref_enc_dat\n",
    "\n",
    "    evs_c = []\n",
    "    resource_c = []\n",
    "    month_c = []\n",
    "    trace_attr = []\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('event'): evs_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('resource'): resource_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('month'): month_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if not (ca in evs_c or ca in resource_c or ca in month_c): trace_attr.append(ca)\n",
    "    print(evs_c)\n",
    "    print(resource_c)\n",
    "    print(month_c)\n",
    "    print(trace_attr)\n",
    "\n",
    "    X_events = {}\n",
    "    X_resource = {}\n",
    "    X_month = {}\n",
    "    X_tracea = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        X_events[prefix] = X_cum[prefix][evs_c]\n",
    "        X_resource[prefix] = X_cum[prefix][resource_c]\n",
    "        X_month[prefix] = X_cum[prefix][month_c]\n",
    "        X_tracea[prefix] = X_cum[prefix][trace_attr]\n",
    "\n",
    "    cat_tas = []\n",
    "    for cat in X_tracea[1].columns:\n",
    "        if type(X_tracea[1][cat][0]) == str:\n",
    "            cat_tas.append(cat)\n",
    "\n",
    "    uniques_cats = {}\n",
    "    for cat in cat_tas:\n",
    "        uniques_cats[cat] = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        for cat in cat_tas:\n",
    "            for reals in list(X_tracea[prefix][cat].unique()):\n",
    "                if not reals in uniques_cats[cat]:\n",
    "                    uniques_cats[cat].append(reals)\n",
    "\n",
    "    for cat in cat_tas:\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for j in list(X_tracea[prefix].index):\n",
    "                X_tracea[prefix][cat][j] = uniques_cats[cat].index(X_tracea[prefix][cat][j])\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    for d in dev:\n",
    "        time_start = time.clock()\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat = pd.get_dummies(imb_ref_enc_dat)\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_events[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_events[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_events[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_events[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_events[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_events[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_event = x_tr\n",
    "                X_test_event = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val_event = x_va\n",
    "                y_val = y_va\n",
    "                X_train_event_c = x_tr_c\n",
    "                X_test_event_c = x_te_c\n",
    "                X_val_event_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train_event = np.append(X_train_event, x_tr, axis=0)\n",
    "                X_test_event = np.append(X_test_event, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val_event = np.append(X_val_event, x_va, axis=0)\n",
    "                X_train_event_c = np.append(X_train_event_c, x_tr_c, axis=0)\n",
    "                X_test_event_c = np.append(X_test_event_c, x_te_c, axis=0)\n",
    "                X_val_event_c = np.append(X_val_event_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_event), len(y_train), len(y_train_c), len(X_val_event), len(y_val), len(y_val_c),\n",
    "              len(X_test_event), len(y_test), len(y_test_c))\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_resource[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_resource[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_resource[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_resource[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_resource[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_resource[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_resource = x_tr\n",
    "                X_test_resource = x_te\n",
    "                X_val_resource = x_va\n",
    "                X_train_resource_c = x_tr_c\n",
    "                X_test_resource_c = x_te_c\n",
    "                X_val_resource_c = x_va_c\n",
    "            else:\n",
    "                X_train_resource = np.append(X_train_resource, x_tr, axis=0)\n",
    "                X_test_resource = np.append(X_test_resource, x_te, axis=0)\n",
    "                X_val_resource = np.append(X_val_resource, x_va, axis=0)\n",
    "                X_train_resource_c = np.append(X_train_resource_c, x_tr_c, axis=0)\n",
    "                X_test_resource_c = np.append(X_test_resource_c, x_te_c, axis=0)\n",
    "                X_val_resource_c = np.append(X_val_resource_c, x_va_c,\n",
    "                                             axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_resource), len(y_train), len(X_val_resource), len(y_val), len(X_test_resource), len(y_test))\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_month[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_month[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_month[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_month[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_month[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_month[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_m = x_tr\n",
    "                X_test_m = x_te\n",
    "                X_val_m = x_va\n",
    "                X_train_m_c = x_tr_c\n",
    "                X_test_m_c = x_te_c\n",
    "                X_val_m_c = x_va_c\n",
    "            else:\n",
    "                X_train_m = np.append(X_train_m, x_tr, axis=0)\n",
    "                X_test_m = np.append(X_test_m, x_te, axis=0)\n",
    "                X_val_m = np.append(X_val_m, x_va, axis=0)\n",
    "                X_train_m_c = np.append(X_train_m_c, x_tr_c, axis=0)\n",
    "                X_test_m_c = np.append(X_test_m_c, x_te_c, axis=0)\n",
    "                X_val_m_c = np.append(X_val_m_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_m), len(y_train), len(X_val_m), len(y_val), len(X_test_m), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_tracea[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_tr = X_tracea[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_va = X_tracea[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "            x_te_c = X_tracea[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_tr_c = X_tracea[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            x_va_c = X_tracea[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train_TA = x_tr\n",
    "                X_test_TA = x_te\n",
    "                X_val_TA = x_va\n",
    "                X_train_TA_c = x_tr_c\n",
    "                X_test_TA_c = x_te_c\n",
    "                X_val_TA_c = x_va_c\n",
    "            else:\n",
    "                X_train_TA = np.append(X_train_TA, x_tr, axis=0)\n",
    "                X_test_TA = np.append(X_test_TA, x_te, axis=0)\n",
    "                X_val_TA = np.append(X_val_TA, x_va, axis=0)\n",
    "                X_train_TA_c = np.append(X_train_TA_c, x_tr_c, axis=0)\n",
    "                X_test_TA_c = np.append(X_test_TA_c, x_te_c, axis=0)\n",
    "                X_val_TA_c = np.append(X_val_TA_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train_TA), len(y_train), len(X_val_TA), len(y_val), len(X_test_TA), len(y_test))\n",
    "\n",
    "        events_encoder = list(\n",
    "            np.unique(np.append(np.append(X_train_event_c, X_test_event_c, axis=0), X_val_event_c, axis=0)))\n",
    "        events_encoder.index('No')\n",
    "        resource_encoder = list(\n",
    "            np.unique(np.append(np.append(X_train_resource_c, X_test_resource_c, axis=0), X_val_resource_c, axis=0)))\n",
    "        resource_encoder.index('No')\n",
    "        cat_ecnoders = {}\n",
    "        for cat in cat_tas:\n",
    "            cat_ecnoders[cat] = list(np.unique(np.append(np.append(X_train_TA_c, X_test_TA_c, axis=0), X_val_TA_c, axis=0)))\n",
    "\n",
    "        month_encoder = list(np.unique(np.append(np.append(X_train_m_c, X_test_m_c, axis=0), X_val_m_c, axis=0)))\n",
    "        month_encoder\n",
    "        for i in range(len(X_test_event)):\n",
    "            for j in range(len(X_test_event[0])):\n",
    "                X_test_event[i][j] = events_encoder.index(X_test_event[i][j])\n",
    "            for j in range(len(X_test_resource[0])):\n",
    "                X_test_resource[i][j] = resource_encoder.index(X_test_resource[i][j])\n",
    "            for j in range(len(X_test_m[0])):\n",
    "                X_test_m[i][j] = month_encoder.index(X_test_m[i][j])\n",
    "        for i in range(len(X_train_event)):\n",
    "            for j in range(len(X_train_event[0])):\n",
    "                X_train_event[i][j] = events_encoder.index(X_train_event[i][j])\n",
    "            for j in range(len(X_train_resource[0])):\n",
    "                X_train_resource[i][j] = resource_encoder.index(X_train_resource[i][j])\n",
    "            for j in range(len(X_train_m[0])):\n",
    "                X_train_m[i][j] = month_encoder.index(X_train_m[i][j])\n",
    "        for i in range(len(X_val_event)):\n",
    "            for j in range(len(X_val_event[0])):\n",
    "                X_val_event[i][j] = events_encoder.index(X_val_event[i][j])\n",
    "            for j in range(len(X_val_resource[0])):\n",
    "                X_val_resource[i][j] = resource_encoder.index(X_val_resource[i][j])\n",
    "            for j in range(len(X_val_m[0])):\n",
    "                X_val_m[i][j] = month_encoder.index(X_val_m[i][j])\n",
    "\n",
    "        # for combs_output\n",
    "        for i in range(len(X_test_event_c)):\n",
    "            for j in range(len(X_test_event_c[0])):\n",
    "                X_test_event_c[i][j] = events_encoder.index(X_test_event_c[i][j])\n",
    "            for j in range(len(X_test_resource[0])):\n",
    "                X_test_resource_c[i][j] = resource_encoder.index(X_test_resource_c[i][j])\n",
    "            for j in range(len(X_test_m[0])):\n",
    "                X_test_m_c[i][j] = month_encoder.index(X_test_m_c[i][j])\n",
    "        for i in range(len(X_train_event_c)):\n",
    "            for j in range(len(X_train_event_c[0])):\n",
    "                X_train_event_c[i][j] = events_encoder.index(X_train_event_c[i][j])\n",
    "            for j in range(len(X_train_resource[0])):\n",
    "                X_train_resource_c[i][j] = resource_encoder.index(X_train_resource_c[i][j])\n",
    "            for j in range(len(X_train_m[0])):\n",
    "                X_train_m_c[i][j] = month_encoder.index(X_train_m_c[i][j])\n",
    "        for i in range(len(X_val_event_c)):\n",
    "            for j in range(len(X_val_event_c[0])):\n",
    "                X_val_event_c[i][j] = events_encoder.index(X_val_event_c[i][j])\n",
    "            for j in range(len(X_val_resource_c[0])):\n",
    "                X_val_resource_c[i][j] = resource_encoder.index(X_val_resource_c[i][j])\n",
    "            for j in range(len(X_val_m_c[0])):\n",
    "                X_val_m_c[i][j] = month_encoder.index(X_val_m_c[i][j])\n",
    "        scaler = StandardScaler()\n",
    "        X_test_TA = scaler.fit_transform(X_test_TA)\n",
    "        X_train_TA = scaler.fit_transform(X_train_TA)\n",
    "        X_val_TA = scaler.fit_transform(X_val_TA)\n",
    "        X_test_TA_c = scaler.fit_transform(X_test_TA_c)\n",
    "        X_train_TA_c = scaler.fit_transform(X_train_TA_c)\n",
    "        X_val_TA_c = scaler.fit_transform(X_val_TA_c)\n",
    "        X_test_event = X_test_event.astype(int)\n",
    "        X_train_event = X_train_event.astype(int)\n",
    "        X_val_event = X_val_event.astype(int)\n",
    "        X_test_resource = X_test_resource.astype(int)\n",
    "        X_train_resource = X_train_resource.astype(int)\n",
    "        X_val_resource = X_val_resource.astype(int)\n",
    "        X_test_m = X_test_m.astype(int)\n",
    "        X_train_m = X_train_m.astype(int)\n",
    "        X_val_m = X_val_m.astype(int)\n",
    "        #for combs again\n",
    "        X_test_event_c = X_test_event_c.astype(int)\n",
    "        X_train_event_c = X_train_event_c.astype(int)\n",
    "        X_val_event_c = X_val_event_c.astype(int)\n",
    "        X_test_resource_c = X_test_resource_c.astype(int)\n",
    "        X_train_resource_c = X_train_resource_c.astype(int)\n",
    "        X_val_resource_c = X_val_resource_c.astype(int)\n",
    "        X_test_m_c = X_test_m_c.astype(int)\n",
    "        X_train_m_c = X_train_m_c.astype(int)\n",
    "        X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BPDP_LSTM(vocab_events=len(events_encoder), vocab_resources=len(resource_encoder), no_TA=len(X_train_TA[0]),\n",
    "                          vocab_month=len(month_encoder))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        torch.autograd.set_detect_anomaly(True)\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data_event = TrainData(torch.FloatTensor(X_train_event),\n",
    "                                         torch.FloatTensor(y_train))\n",
    "            train_data_resource = TestData(torch.FloatTensor(X_train_resource))\n",
    "            train_data_TA = TestData(torch.FloatTensor(X_train_TA))\n",
    "            train_data_m = TestData(torch.FloatTensor(X_train_m))\n",
    "            train_loader_event = DataLoader(dataset=train_data_event, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_resource = DataLoader(dataset=train_data_resource, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_TA = DataLoader(dataset=train_data_TA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "            train_loader_m = DataLoader(dataset=train_data_m, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader_event))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                steps_r = list((train_loader_resource))\n",
    "                steps_ta = list((train_loader_TA))\n",
    "                steps_m = list((train_loader_m))\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                         steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(torch.FloatTensor(X_val_event).to(torch.int64),\n",
    "                                     torch.FloatTensor(X_val_resource).to(torch.int64),\n",
    "                                     torch.FloatTensor(X_val_TA).to(torch.float),\n",
    "                                     torch.FloatTensor(X_val_m).to(torch.int64))\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader_event)}, Acc: {epoch_acc / len(train_loader_event):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader_event):}, Acc: {epoch_acc / len(train_loader_event):.3f}\")\n",
    "\n",
    "        y_batch_pred\n",
    "        model.eval()\n",
    "        test_data_event = TestData(torch.FloatTensor(X_test_event))\n",
    "        test_data_resource = TestData(torch.FloatTensor(X_test_resource))\n",
    "        test_data_TA = TestData(torch.FloatTensor(X_test_TA))\n",
    "        test_data_m = TestData(torch.FloatTensor(X_test_m))\n",
    "        test_loader_event = DataLoader(dataset=test_data_event, batch_size=1)\n",
    "        test_loader_resource = DataLoader(dataset=test_data_resource, batch_size=1)\n",
    "        test_loader_TA = DataLoader(dataset=test_data_TA, batch_size=1)\n",
    "        test_loader_m = DataLoader(dataset=test_data_m, batch_size=1)\n",
    "        iterations_r = iter(test_loader_resource)\n",
    "        iterations_ta = iter(test_loader_TA)\n",
    "        iterations_m = iter(test_loader_m)\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, X_batch in enumerate(test_loader_event):\n",
    "                X_batch = X_batch.to(device).to(torch.int64)\n",
    "                y_test_pred = torch.nn.functional.softmax(\n",
    "                    model(X_batch, next(iterations_r).to(torch.int64), next(iterations_ta).to(torch.float),\n",
    "                          next(iterations_m).to(torch.int64)))\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        time_elapsed = time_start - time.clock()\n",
    "        metrics[d]['Time']=time_elapsed\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        print(metrics)\n",
    "        models_collect[d] = model\n",
    "\n",
    "        X_test_event_c = X_test_event_c.astype(int)\n",
    "        X_train_event_c = X_train_event_c.astype(int)\n",
    "        X_val_event_c = X_val_event_c.astype(int)\n",
    "        X_test_resource_c = X_test_resource_c.astype(int)\n",
    "        X_train_resource_c = X_train_resource_c.astype(int)\n",
    "        X_val_resource_c = X_val_resource_c.astype(int)\n",
    "        X_test_m_c = X_test_m_c.astype(int)\n",
    "        X_train_m_c = X_train_m_c.astype(int)\n",
    "        X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "        train_data_f_combs_event = TestData(torch.FloatTensor(X_train_event_c))\n",
    "        train_loader_f_combs_event = DataLoader(dataset=train_data_f_combs_event, batch_size=len(X_train_event_c))\n",
    "        train_data_f_combs_resource = TestData(torch.FloatTensor(X_train_resource_c))\n",
    "        train_loader_f_combs_resource = DataLoader(dataset=train_data_f_combs_resource, batch_size=len(X_train_resource_c))\n",
    "        train_data_f_combs_m = TestData(torch.FloatTensor(X_train_m_c))\n",
    "        train_loader_f_combs_m = DataLoader(dataset=train_data_f_combs_m, batch_size=len(X_train_m_c))\n",
    "        train_data_f_combs_TA = TestData(torch.FloatTensor(X_train_TA_c))\n",
    "        train_loader_f_combs_TA = DataLoader(dataset=train_data_f_combs_TA, batch_size=len(X_train_TA_c))\n",
    "\n",
    "        y_output_train = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((train_loader_f_combs_resource))\n",
    "            steps_ta = list((train_loader_f_combs_TA))\n",
    "            steps_m = list((train_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(train_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "        test_data_f_combs_event = TestData(torch.FloatTensor(X_test_event_c))\n",
    "        test_loader_f_combs_event = DataLoader(dataset=test_data_f_combs_event, batch_size=len(X_test_event_c))\n",
    "        test_data_f_combs_resource = TestData(torch.FloatTensor(X_test_resource_c))\n",
    "        test_loader_f_combs_resource = DataLoader(dataset=test_data_f_combs_resource, batch_size=len(X_test_resource_c))\n",
    "        test_data_f_combs_m = TestData(torch.FloatTensor(X_test_m_c))\n",
    "        test_loader_f_combs_m = DataLoader(dataset=test_data_f_combs_m, batch_size=len(X_test_m_c))\n",
    "        test_data_f_combs_TA = TestData(torch.FloatTensor(X_test_TA_c))\n",
    "        test_loader_f_combs_TA = DataLoader(dataset=test_data_f_combs_TA, batch_size=len(X_test_TA_c))\n",
    "\n",
    "        y_output_test = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((test_loader_f_combs_resource))\n",
    "            steps_ta = list((test_loader_f_combs_TA))\n",
    "            steps_m = list((test_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(test_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "        val_data_f_combs_event = TestData(torch.FloatTensor(X_val_event_c))\n",
    "        val_loader_f_combs_event = DataLoader(dataset=val_data_f_combs_event, batch_size=len(X_val_event_c))\n",
    "        val_data_f_combs_resource = TestData(torch.FloatTensor(X_val_resource_c))\n",
    "        val_loader_f_combs_resource = DataLoader(dataset=val_data_f_combs_resource, batch_size=len(X_val_resource_c))\n",
    "        val_data_f_combs_m = TestData(torch.FloatTensor(X_val_m_c))\n",
    "        val_loader_f_combs_m = DataLoader(dataset=val_data_f_combs_m, batch_size=len(X_val_m_c))\n",
    "        val_data_f_combs_TA = TestData(torch.FloatTensor(X_val_TA_c))\n",
    "        val_loader_f_combs_TA = DataLoader(dataset=val_data_f_combs_TA, batch_size=len(X_val_TA_c))\n",
    "\n",
    "        y_output_val = []\n",
    "        with torch.no_grad():\n",
    "            steps_r = list((val_loader_f_combs_resource))\n",
    "            steps_ta = list((val_loader_f_combs_TA))\n",
    "            steps_m = list((val_loader_f_combs_m))\n",
    "            for i, X_batch in enumerate(val_loader_f_combs_event):\n",
    "                y_test_pred = model(X_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                    steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "                y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "        outputs_train['NoDev' + str(d)] = y_output_train[0][:, 0]\n",
    "        outputs_train['Dev' + str(d)] = y_output_train[0][:, 1]\n",
    "        outputs_test['NoDev' + str(d)] = y_output_test[0][:, 0]\n",
    "        outputs_test['Dev' + str(d)] = y_output_test[0][:, 1]\n",
    "        outputs_val['NoDev' + str(d)] = y_output_val[0][:, 0]\n",
    "        outputs_val['Dev' + str(d)] = y_output_val[0][:, 1]\n",
    "        if d == dev[0]:\n",
    "            outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "            outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "            outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_LSTM_CIBE(log, ref_log, aligned_traces)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_collective_LSTM_CIBE(log, ref_log, aligned_traces, u_sample = True,early_stop = True,explained = False,split = 1 / 3):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "\n",
    "    ref_enc_dat = ref_clean_dat.copy()\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.00001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = clean_dat.copy()\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 'No'  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value='No')\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_LSTM')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    ref_enc_dat\n",
    "\n",
    "    evs_c = []\n",
    "    resource_c = []\n",
    "    month_c = []\n",
    "    trace_attr = []\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('event'): evs_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('resource'): resource_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if ca.startswith('month'): month_c.append(ca)\n",
    "    for ca in X_cum[1].columns:\n",
    "        if not (ca in evs_c or ca in resource_c or ca in month_c): trace_attr.append(ca)\n",
    "    print(evs_c)\n",
    "    print(resource_c)\n",
    "    print(month_c)\n",
    "    print(trace_attr)\n",
    "\n",
    "    X_events = {}\n",
    "    X_resource = {}\n",
    "    X_month = {}\n",
    "    X_tracea = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        X_events[prefix] = X_cum[prefix][evs_c]\n",
    "        X_resource[prefix] = X_cum[prefix][resource_c]\n",
    "        X_month[prefix] = X_cum[prefix][month_c]\n",
    "        X_tracea[prefix] = X_cum[prefix][trace_attr]\n",
    "\n",
    "    cat_tas = []\n",
    "    for cat in X_tracea[1].columns:\n",
    "        if type(X_tracea[1][cat][0]) == str:\n",
    "            cat_tas.append(cat)\n",
    "    cat_tas\n",
    "    uniques_cats = {}\n",
    "    for cat in cat_tas:\n",
    "        uniques_cats[cat] = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        for cat in cat_tas:\n",
    "            for reals in list(X_tracea[prefix][cat].unique()):\n",
    "                if not reals in uniques_cats[cat]:\n",
    "                    uniques_cats[cat].append(reals)\n",
    "    uniques_cats\n",
    "    X_tracea[1]\n",
    "    for cat in cat_tas:\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for j in list(X_tracea[prefix].index):\n",
    "                X_tracea[prefix][cat][j] = uniques_cats[cat].index(X_tracea[prefix][cat][j])\n",
    "    X_tracea[1]\n",
    "    X_events[1]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 8\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    Y_cum_dev = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "        Y_cum_dev[prefix]['NoDev'] = 0\n",
    "        for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "            Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "        if prefix == 1:\n",
    "            print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "    print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "    # validation set for early stopping\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "\n",
    "                                                                      random_state=0)\n",
    "\n",
    "    enumerated_trace_idx = {}\n",
    "    cum_trace_idxs = []\n",
    "    pref_list = []\n",
    "    pref_list_train_c = []\n",
    "    pref_list_test_c = []\n",
    "    pref_list_val_c = []\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_events[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_events[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_events[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_te_c = X_events[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_events[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_events[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "            [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "            float)\n",
    "        cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "        pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "        pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "        pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "        pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_event = x_tr\n",
    "            X_test_event = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val_event = x_va\n",
    "            y_val = y_va\n",
    "            X_train_event_c = x_tr_c\n",
    "            X_test_event_c = x_te_c\n",
    "            X_val_event_c = x_va_c\n",
    "            y_train_c = y_tr_c\n",
    "            y_test_c = y_te_c\n",
    "            y_val_c = y_va_c\n",
    "        else:\n",
    "            X_train_event = np.append(X_train_event, x_tr, axis=0)\n",
    "            X_test_event = np.append(X_test_event, x_te, axis=0)\n",
    "            y_train = np.append(y_train, y_tr, axis=0)\n",
    "            y_test = np.append(y_test, y_te, axis=0)\n",
    "            y_val = np.append(y_val, y_va, axis=0)\n",
    "            X_val_event = np.append(X_val_event, x_va, axis=0)\n",
    "            X_train_event_c = np.append(X_train_event_c, x_tr_c, axis=0)\n",
    "            X_test_event_c = np.append(X_test_event_c, x_te_c, axis=0)\n",
    "            X_val_event_c = np.append(X_val_event_c, x_va_c, axis=0)\n",
    "            y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "            y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "            y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_event), len(y_train), len(y_train_c), len(X_val_event), len(y_val), len(y_val_c),\n",
    "          len(X_test_event), len(y_test), len(y_test_c))\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_resource[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_resource[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_resource[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_resource[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_resource[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_resource[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_resource = x_tr\n",
    "            X_test_resource = x_te\n",
    "            X_val_resource = x_va\n",
    "            X_train_resource_c = x_tr_c\n",
    "            X_test_resource_c = x_te_c\n",
    "            X_val_resource_c = x_va_c\n",
    "        else:\n",
    "            X_train_resource = np.append(X_train_resource, x_tr, axis=0)\n",
    "            X_test_resource = np.append(X_test_resource, x_te, axis=0)\n",
    "            X_val_resource = np.append(X_val_resource, x_va, axis=0)\n",
    "            X_train_resource_c = np.append(X_train_resource_c, x_tr_c, axis=0)\n",
    "            X_test_resource_c = np.append(X_test_resource_c, x_te_c, axis=0)\n",
    "            X_val_resource_c = np.append(X_val_resource_c, x_va_c,\n",
    "                                         axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_resource), len(y_train), len(X_val_resource), len(y_val), len(X_test_resource), len(y_test))\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_month[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_month[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_month[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_month[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_month[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_month[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_m = x_tr\n",
    "            X_test_m = x_te\n",
    "            X_val_m = x_va\n",
    "            X_train_m_c = x_tr_c\n",
    "            X_test_m_c = x_te_c\n",
    "            X_val_m_c = x_va_c\n",
    "        else:\n",
    "            X_train_m = np.append(X_train_m, x_tr, axis=0)\n",
    "            X_test_m = np.append(X_test_m, x_te, axis=0)\n",
    "            X_val_m = np.append(X_val_m, x_va, axis=0)\n",
    "            X_train_m_c = np.append(X_train_m_c, x_tr_c, axis=0)\n",
    "            X_test_m_c = np.append(X_test_m_c, x_te_c, axis=0)\n",
    "            X_val_m_c = np.append(X_val_m_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_m), len(y_train), len(X_val_m), len(y_val), len(X_test_m), len(y_test))\n",
    "\n",
    "    print('split done')\n",
    "\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te = X_tracea[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_tr = X_tracea[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_va = X_tracea[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy()\n",
    "        x_te_c = X_tracea[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_tr_c = X_tracea[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        x_va_c = X_tracea[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy()\n",
    "        print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "        if prefix == 1:\n",
    "            X_train_TA = x_tr\n",
    "            X_test_TA = x_te\n",
    "            X_val_TA = x_va\n",
    "            X_train_TA_c = x_tr_c\n",
    "            X_test_TA_c = x_te_c\n",
    "            X_val_TA_c = x_va_c\n",
    "        else:\n",
    "            X_train_TA = np.append(X_train_TA, x_tr, axis=0)\n",
    "            X_test_TA = np.append(X_test_TA, x_te, axis=0)\n",
    "            X_val_TA = np.append(X_val_TA, x_va, axis=0)\n",
    "            X_train_TA_c = np.append(X_train_TA_c, x_tr_c, axis=0)\n",
    "            X_test_TA_c = np.append(X_test_TA_c, x_te_c, axis=0)\n",
    "            X_val_TA_c = np.append(X_val_TA_c, x_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train_TA), len(y_train), len(X_val_TA), len(y_val), len(X_test_TA), len(y_test))\n",
    "\n",
    "    events_encoder = list(\n",
    "        np.unique(np.append(np.append(X_train_event_c, X_test_event_c, axis=0), X_val_event_c, axis=0)))\n",
    "    events_encoder.index('No')\n",
    "    resource_encoder = list(\n",
    "        np.unique(np.append(np.append(X_train_resource_c, X_test_resource_c, axis=0), X_val_resource_c, axis=0)))\n",
    "    resource_encoder.index('No')\n",
    "    cat_ecnoders = {}\n",
    "    for cat in cat_tas:\n",
    "        cat_ecnoders[cat] = list(np.unique(np.append(np.append(X_train_TA_c, X_test_TA_c, axis=0), X_val_TA_c, axis=0)))\n",
    "\n",
    "    month_encoder = list(np.unique(np.append(np.append(X_train_m_c, X_test_m_c, axis=0), X_val_m_c, axis=0)))\n",
    "    month_encoder\n",
    "    for i in range(len(X_test_event)):\n",
    "        for j in range(len(X_test_event[0])):\n",
    "            X_test_event[i][j] = events_encoder.index(X_test_event[i][j])\n",
    "        for j in range(len(X_test_resource[0])):\n",
    "            X_test_resource[i][j] = resource_encoder.index(X_test_resource[i][j])\n",
    "        for j in range(len(X_test_m[0])):\n",
    "            X_test_m[i][j] = month_encoder.index(X_test_m[i][j])\n",
    "    for i in range(len(X_train_event)):\n",
    "        for j in range(len(X_train_event[0])):\n",
    "            X_train_event[i][j] = events_encoder.index(X_train_event[i][j])\n",
    "        for j in range(len(X_train_resource[0])):\n",
    "            X_train_resource[i][j] = resource_encoder.index(X_train_resource[i][j])\n",
    "        for j in range(len(X_train_m[0])):\n",
    "            X_train_m[i][j] = month_encoder.index(X_train_m[i][j])\n",
    "    for i in range(len(X_val_event)):\n",
    "        for j in range(len(X_val_event[0])):\n",
    "            X_val_event[i][j] = events_encoder.index(X_val_event[i][j])\n",
    "        for j in range(len(X_val_resource[0])):\n",
    "            X_val_resource[i][j] = resource_encoder.index(X_val_resource[i][j])\n",
    "        for j in range(len(X_val_m[0])):\n",
    "            X_val_m[i][j] = month_encoder.index(X_val_m[i][j])\n",
    "\n",
    "    # for combs_output\n",
    "    for i in range(len(X_test_event_c)):\n",
    "        for j in range(len(X_test_event_c[0])):\n",
    "            X_test_event_c[i][j] = events_encoder.index(X_test_event_c[i][j])\n",
    "        for j in range(len(X_test_resource[0])):\n",
    "            X_test_resource_c[i][j] = resource_encoder.index(X_test_resource_c[i][j])\n",
    "        for j in range(len(X_test_m[0])):\n",
    "            X_test_m_c[i][j] = month_encoder.index(X_test_m_c[i][j])\n",
    "    for i in range(len(X_train_event_c)):\n",
    "        for j in range(len(X_train_event_c[0])):\n",
    "            X_train_event_c[i][j] = events_encoder.index(X_train_event_c[i][j])\n",
    "        for j in range(len(X_train_resource[0])):\n",
    "            X_train_resource_c[i][j] = resource_encoder.index(X_train_resource_c[i][j])\n",
    "        for j in range(len(X_train_m[0])):\n",
    "            X_train_m_c[i][j] = month_encoder.index(X_train_m_c[i][j])\n",
    "    for i in range(len(X_val_event_c)):\n",
    "        for j in range(len(X_val_event_c[0])):\n",
    "            X_val_event_c[i][j] = events_encoder.index(X_val_event_c[i][j])\n",
    "        for j in range(len(X_val_resource_c[0])):\n",
    "            X_val_resource_c[i][j] = resource_encoder.index(X_val_resource_c[i][j])\n",
    "        for j in range(len(X_val_m_c[0])):\n",
    "            X_val_m_c[i][j] = month_encoder.index(X_val_m_c[i][j])\n",
    "    scaler = StandardScaler()\n",
    "    X_test_TA = scaler.fit_transform(X_test_TA)\n",
    "    X_train_TA = scaler.fit_transform(X_train_TA)\n",
    "    X_val_TA = scaler.fit_transform(X_val_TA)\n",
    "    X_test_TA_c = scaler.fit_transform(X_test_TA_c)\n",
    "    X_train_TA_c = scaler.fit_transform(X_train_TA_c)\n",
    "    X_val_TA_c = scaler.fit_transform(X_val_TA_c)\n",
    "    X_test_event = X_test_event.astype(int)\n",
    "    X_train_event = X_train_event.astype(int)\n",
    "    X_val_event = X_val_event.astype(int)\n",
    "    X_test_resource = X_test_resource.astype(int)\n",
    "    X_train_resource = X_train_resource.astype(int)\n",
    "    X_val_resource = X_val_resource.astype(int)\n",
    "    X_test_m = X_test_m.astype(int)\n",
    "    X_train_m = X_train_m.astype(int)\n",
    "    X_val_m = X_val_m.astype(int)\n",
    "    #for combs again\n",
    "    X_test_event_c = X_test_event_c.astype(int)\n",
    "    X_train_event_c = X_train_event_c.astype(int)\n",
    "    X_val_event_c = X_val_event_c.astype(int)\n",
    "    X_test_resource_c = X_test_resource_c.astype(int)\n",
    "    X_train_resource_c = X_train_resource_c.astype(int)\n",
    "    X_val_resource_c = X_val_resource_c.astype(int)\n",
    "    X_test_m_c = X_test_m_c.astype(int)\n",
    "    X_train_m_c = X_train_m_c.astype(int)\n",
    "    X_val_m_c = X_val_m_c.astype(int)\n",
    "\n",
    "    y_train\n",
    "\n",
    "\n",
    "    class BPDP_LSTM_SC(nn.Module):\n",
    "        def __init__(self, vocab_events, vocab_resources, no_TA, vocab_month, no_devs):\n",
    "            super(BPDP_LSTM_SC, self).__init__()\n",
    "            self.embedding_e = nn.Embedding(vocab_events, 16)  # hier auf 8 / 16\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.lstm_e = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_e = nn.Linear(64, 32)\n",
    "            self.embedding_r = nn.Embedding(vocab_resources, 16)\n",
    "            self.lstm_r = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_r = nn.Linear(64, 32)\n",
    "            self.embedding_m = nn.Embedding(vocab_month, 16)\n",
    "            self.lstm_m = nn.LSTM(input_size=16, hidden_size=64, num_layers=1, batch_first=True, dropout=0.1)\n",
    "            self.linear_m = nn.Linear(64, 32)\n",
    "            self.linear_ta = nn.Linear(no_TA, 32)\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(128)\n",
    "            self.linear = nn.Linear(128, no_devs)\n",
    "\n",
    "        def forward(self, evs, rs, tas, ms):\n",
    "            evs = self.embedding_e(evs)\n",
    "            evs, _ = self.lstm_e(evs)\n",
    "            evs = self.linear_e(evs)\n",
    "            evs = evs[:, -1, :]\n",
    "            evs = self.activation1(evs)\n",
    "            rs = self.embedding_r(rs)\n",
    "            rs, _ = self.lstm_r(rs)\n",
    "            rs = rs[:, -1, :]\n",
    "            rs = self.activation1(rs)\n",
    "            rs = self.linear_r(rs)\n",
    "            ms = self.embedding_m(ms)\n",
    "            ms, _ = self.lstm_m(ms)\n",
    "            ms = ms[:, -1, :]\n",
    "            ms = self.activation1(ms)\n",
    "            ms = self.linear_m(ms)\n",
    "            tas = self.linear_ta(tas)\n",
    "            fin = torch.cat((evs, rs), dim=1)\n",
    "            fin = torch.cat((fin, ms), dim=1)\n",
    "            fin = torch.cat((fin, tas), dim=1)\n",
    "            fin = self.batchnorm1(fin)\n",
    "            #fin = self.dropout(fin)\n",
    "            fin = self.linear(fin)\n",
    "            return fin\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (2 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    positive_weights\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BPDP_LSTM_SC(vocab_events=len(events_encoder), vocab_resources=len(resource_encoder), no_TA=len(X_train_TA[0]),\n",
    "                         vocab_month=len(month_encoder), no_devs=len(y_train[0]))\n",
    "    model.to(device)\n",
    "    weights = torch.FloatTensor(list(positive_weights.values()))\n",
    "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "    if early_stop:\n",
    "        EPOCHS = 100\n",
    "        model.train()\n",
    "        train_data_event = TrainData(torch.FloatTensor(X_train_event),\n",
    "                                     torch.FloatTensor(y_train))\n",
    "        train_data_resource = TestData(torch.FloatTensor(X_train_resource))\n",
    "        train_data_TA = TestData(torch.FloatTensor(X_train_TA))\n",
    "        train_data_m = TestData(torch.FloatTensor(X_train_m))\n",
    "        train_loader_event = DataLoader(dataset=train_data_event, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_resource = DataLoader(dataset=train_data_resource, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_TA = DataLoader(dataset=train_data_TA, batch_size=BATCH_SIZE, shuffle=False)\n",
    "        train_loader_m = DataLoader(dataset=train_data_m, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader_event))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            steps_r = list((train_loader_resource))\n",
    "            steps_ta = list((train_loader_TA))\n",
    "            steps_m = list((train_loader_m))\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(torch.int64).to(device), steps_r[i].to(torch.int64).to(device),\n",
    "                                     steps_ta[i].to(torch.float).to(device), steps_m[i].to(torch.int64).to(device))\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "                loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                epoch_loss += loss\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(torch.FloatTensor(X_val_event).to(torch.int64),\n",
    "                                 torch.FloatTensor(X_val_resource).to(torch.int64),\n",
    "                                 torch.FloatTensor(X_val_TA).to(torch.float),\n",
    "                                 torch.FloatTensor(X_val_m).to(torch.int64))\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader_event)}, Acc: {epoch_acc / len(train_loader_event):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader_event):}, Acc: {epoch_acc / len(train_loader_event):.3f}\")\n",
    "\n",
    "    y_batch_pred\n",
    "    model.eval()\n",
    "    test_data_event = TestData(torch.FloatTensor(X_test_event))\n",
    "    test_data_resource = TestData(torch.FloatTensor(X_test_resource))\n",
    "    test_data_TA = TestData(torch.FloatTensor(X_test_TA))\n",
    "    test_data_m = TestData(torch.FloatTensor(X_test_m))\n",
    "    test_loader_event = DataLoader(dataset=test_data_event, batch_size=1)\n",
    "    test_loader_resource = DataLoader(dataset=test_data_resource, batch_size=1)\n",
    "    test_loader_TA = DataLoader(dataset=test_data_TA, batch_size=1)\n",
    "    test_loader_m = DataLoader(dataset=test_data_m, batch_size=1)\n",
    "    iterations_r = iter(test_loader_resource)\n",
    "    iterations_ta = iter(test_loader_TA)\n",
    "    iterations_m = iter(test_loader_m)\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, X_batch in enumerate(test_loader_event):\n",
    "            X_batch = X_batch.to(device).to(torch.int64)\n",
    "            y_test_pred = torch.nn.functional.sigmoid(\n",
    "                model(X_batch, next(iterations_r).to(torch.int64), next(iterations_ta).to(torch.float),\n",
    "                      next(iterations_m).to(torch.int64)))\n",
    "            y_pred_tag = torch.round(y_test_pred)\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    for i, d in enumerate(dev):\n",
    "        metrics[d]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[d]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        metrics[d]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                  average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    print(CM)\n",
    "\n",
    "    print(metrics)\n",
    "    writer = pd.ExcelWriter('BPDP_LSTM/' + z + '_BPDP_LSTM_SC_1.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_DPP_CIBE(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "    models_collect = {}\n",
    "    dev_trained = []\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_cum[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr_c = X_cum[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va_c = X_cum[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "                X_train_c = x_tr_c\n",
    "                X_test_c = x_te_c\n",
    "                X_val_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)\n",
    "                X_train_c = np.append(X_train_c, x_tr_c, axis=0)\n",
    "                X_test_c = np.append(X_test_c, x_te_c, axis=0)\n",
    "                X_val_c = np.append(X_val_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(y_train_c), len(X_val), len(y_val), len(y_val_c), len(X_test), len(y_test),\n",
    "              len(y_test_c))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "        X_test_c = scaler.fit_transform(X_test_c)\n",
    "        X_train_c = scaler.fit_transform(X_train_c)\n",
    "        X_val_c = scaler.fit_transform(X_val_c)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "        pref_list = flatten_comprehension(pref_list)\n",
    "        if d == dev[0]:\n",
    "            y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_test_cum['prefix_length'] = pref_list\n",
    "            y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_pred_cum['prefix_length'] = pref_list\n",
    "        y_pred_cum[str('confidence' + d)] = y_confidence_list\n",
    "        y_test_cum[d] = list(y_test)\n",
    "        y_pred_cum[d] = y_pred_list\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        print(metrics)\n",
    "        models_collect[d] = model\n",
    "\n",
    "        train_data_f_combs = TestData(torch.FloatTensor(X_train_c))\n",
    "        train_loader_f_combs = DataLoader(dataset=train_data_f_combs, batch_size=len(X_train_c))\n",
    "\n",
    "        y_output_train = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in train_loader_f_combs:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_output_train.append(y_test_pred.numpy())\n",
    "\n",
    "        test_data_f_combs = TestData(torch.FloatTensor(X_test_c))\n",
    "        test_loader_f_combs = DataLoader(dataset=test_data_f_combs, batch_size=len(X_test_c))\n",
    "\n",
    "        y_output_test = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader_f_combs:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_output_test.append(y_test_pred.numpy())\n",
    "\n",
    "        val_data_f_combs = TestData(torch.FloatTensor(X_val_c))\n",
    "        val_loader_f_combs = DataLoader(dataset=val_data_f_combs, batch_size=len(X_val_c))\n",
    "\n",
    "        y_output_val = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in val_loader_f_combs:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_output_val.append(y_test_pred.numpy())\n",
    "\n",
    "        outputs_train['NoDev' + str(d)] = y_output_train[0][:, 0]\n",
    "        outputs_train['Dev' + str(d)] = y_output_train[0][:, 1]\n",
    "        outputs_test['NoDev' + str(d)] = y_output_test[0][:, 0]\n",
    "        outputs_test['Dev' + str(d)] = y_output_test[0][:, 1]\n",
    "        outputs_val['NoDev' + str(d)] = y_output_val[0][:, 0]\n",
    "        outputs_val['Dev' + str(d)] = y_output_val[0][:, 1]\n",
    "        if d == dev[0]:\n",
    "            outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "            outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "            outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "    outputs_test\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()\n",
    "    y_test_c.sum()\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs(nn.Module):\n",
    "        def __init__(self, no_columns, no_devs):\n",
    "            super(Ensemble_Stack_Combs, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 64)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(128, 64)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(64, no_devs)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(64)\n",
    "            self.batchnorm2 = nn.LayerNorm(64)\n",
    "            self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.layer_1(inputs)\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            #x = self.layer_2(x)\n",
    "            #x = self.activation2(self.layer_2(x))\n",
    "            #x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            #x = self.Sigmoid(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    class Ensemble_Stack_Combs_Single(nn.Module):\n",
    "        def __init__(self, no_columns):\n",
    "            super(Ensemble_Stack_Combs_Single, self).__init__()\n",
    "            self.layer_1 = nn.Linear(no_columns, 512)\n",
    "            self.activation1 = nn.LeakyReLU()\n",
    "            self.layer_2 = nn.Linear(512, 128)\n",
    "            self.activation2 = nn.LeakyReLU()\n",
    "            self.layer_out = nn.Linear(128, 2)\n",
    "\n",
    "            self.dropout = nn.Dropout(p=0.1)\n",
    "            self.batchnorm1 = nn.LayerNorm(512)\n",
    "            self.batchnorm2 = nn.LayerNorm(128)\n",
    "\n",
    "        def forward(self, inputs):\n",
    "            x = self.activation1(self.layer_1(inputs))\n",
    "            x = self.batchnorm1(x)\n",
    "            x = self.activation2(self.layer_2(x))\n",
    "            x = self.batchnorm2(x)\n",
    "            x = self.dropout(x)\n",
    "            x = self.layer_out(x)\n",
    "            return x\n",
    "\n",
    "\n",
    "    len(y_train_c)\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "\n",
    "        model = Ensemble_Stack_Combs_Single(no_columns=len(outputs_train_us[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor([8, 1])\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            y_train_c_us['NoDev']=0\n",
    "            for je in list(y_train_c_us.index):\n",
    "                y_train_c_us['NoDev'][je]=1-y_train_c_us[i][je]\n",
    "            y_val = np.array(y_val_c[:, i], )\n",
    "            y_val = np.column_stack((y_val, 1 - y_val))\n",
    "            y_test = np.array(y_test_c[:, i], )\n",
    "            y_test = np.column_stack((y_test, 1 - y_test))\n",
    "\n",
    "            if early_stop:\n",
    "                EPOCHS = 300\n",
    "                model.train()\n",
    "                train_data = TrainData(torch.FloatTensor(outputs_train_us),\n",
    "                                       torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "\n",
    "                train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "                X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "                es = EarlyStopping()\n",
    "                done = False\n",
    "\n",
    "                epoch = 0\n",
    "                while epoch < EPOCHS and not done:\n",
    "                    epoch += 1\n",
    "                    steps = list(enumerate(train_loader))\n",
    "                    pbar = tqdm.tqdm(steps)\n",
    "                    model.train()\n",
    "                    epoch_acc = 0\n",
    "                    epoch_loss = 0\n",
    "                    for i, (x_batch, y_batch) in pbar:\n",
    "                        optimizer.zero_grad()\n",
    "                        y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                        loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                        acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_acc += acc.item()\n",
    "\n",
    "                        loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                        epoch_loss += loss\n",
    "                        if i == len(steps) - 1:\n",
    "                            model.eval()\n",
    "                            pred = model(X_val)\n",
    "                            vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                            if es(model, vloss): done = True\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                        else:\n",
    "                            pbar.set_description(\n",
    "                                f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "            model.eval()\n",
    "            test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "            test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "            y_pred_list = []\n",
    "            y_confidence_list = []\n",
    "            with torch.no_grad():\n",
    "                for X_batch in test_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                    y_confidence_list.append(y_test_pred.numpy())\n",
    "                    y_pred_tag = torch.round(y_test_pred)\n",
    "                    y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "            y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "            print(CM)\n",
    "\n",
    "            metrics_comb[urc]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics_comb[urc]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "    else:\n",
    "        df_y_train = pd.DataFrame(y_train_c)\n",
    "        df_y_train['dev'] = 0\n",
    "        for a in range(len(df_y_train)):\n",
    "            df_y_train['dev'][a] = max(df_y_train.loc[a])\n",
    "        df_y_train\n",
    "        df_X_train = pd.DataFrame(outputs_train)\n",
    "        df_X_train['ind'] = 0\n",
    "        for ew in range(len(df_X_train)):\n",
    "            df_X_train['ind'][ew] = ew\n",
    "        df_X_train\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "        x_resampled, y_resampled = oss.fit_resample(df_X_train, df_y_train['dev'])\n",
    "        x_resampled\n",
    "        outputs_train_us = df_X_train.loc[list(x_resampled['ind'])].drop('ind', axis=1)\n",
    "        scaler = StandardScaler()\n",
    "        outputs_train_us = scaler.fit_transform(outputs_train_us)\n",
    "        outputs_val = scaler.fit_transform(outputs_val)\n",
    "        outputs_test = scaler.fit_transform(outputs_test)\n",
    "        y_train_c_us = df_y_train.loc[list(x_resampled['ind'])].drop('dev', axis=1)\n",
    "        outputs_train_us\n",
    "        model = Ensemble_Stack_Combs(no_columns=len(outputs_train_us[0]),\n",
    "                                     no_devs=len(y_train_c_us.loc[list(y_train_c_us.index)[0]]))\n",
    "        #model = Ensemble_Stack_Combs(no_columns=len(outputs_train.loc[0]), no_devs=len(y_train_c[0]))\n",
    "        model.to(device)\n",
    "        criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(list(pw_combs.values())))\n",
    "        #criterion = nn.MultiLabelSoftMarginLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(outputs_train_us), torch.FloatTensor(y_train_c_us.to_numpy()))\n",
    "            #train_data = TrainData(torch.FloatTensor(outputs_train.to_numpy()),torch.FloatTensor(y_train_c))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(outputs_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val_c))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(outputs_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.sigmoid(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "        print(CM)\n",
    "        for i, urc in enumerate(unique_rel_combs):\n",
    "            metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "            metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "            metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "            try:\n",
    "                metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                             average='macro')\n",
    "            except Exception as er:\n",
    "                metrics_comb[urc]['ROC_AUC'] = er\n",
    "            metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "            metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "            metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    metrics_comb\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_BPDP_stacked_CIBE_combinations_FFN_w32.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_DPP_CIBE(log, ref_log, aligned_traces)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_separate_CIBE_confidence(log, ref_log, aligned_traces, split=1/3, u_sample=True, early_stop=True,explained=False):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log,\n",
    "                                     4000)  # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')  # output path\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    dev_trained = []\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        # validation set for early stopping\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "        y_confidence_list = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                y_confidence_list.append(y_test_pred.numpy())\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        cum_trace_idxs = flatten_comprehension(cum_trace_idxs)\n",
    "        pref_list = flatten_comprehension(pref_list)\n",
    "        if d == dev[0]:\n",
    "            y_test_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_test_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_test_cum['prefix_length'] = pref_list\n",
    "            y_pred_cum = pd.DataFrame(data=0, columns=dev, index=range(len(y_test)))\n",
    "            y_pred_cum['trace_idx'] = cum_trace_idxs\n",
    "            y_pred_cum['prefix_length'] = pref_list\n",
    "        y_pred_cum[str('confidence' + d)] = y_confidence_list\n",
    "        y_test_cum[d] = list(y_test)\n",
    "        y_pred_cum[d] = y_pred_list\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        print(CM)\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx] >= prefix:\n",
    "                    if prefix == 1:\n",
    "                        dev_position_pred[d][idx] = y_pred_list[to_be_checked_idx[idx][prefix - 1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix - 1]][0] == 1 and \\\n",
    "                                y_pred_list[to_be_checked_idx[idx][prefix - 2]][0] == 0:\n",
    "                            if dev_position[d][idx] <= prefix:\n",
    "                                dev_position_pred[d][idx] = dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx] = prefix\n",
    "\n",
    "        earliness[d] = 0\n",
    "        tobe_devs = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] == 0 or dev_position_pred[d][idx] == 0:\n",
    "                continue\n",
    "            tobe_devs += 1\n",
    "            earliness[d] += dev_position_pred[d][idx] / dev_position[d][idx]\n",
    "        if not tobe_devs == 0:\n",
    "            earliness[d] = earliness[d] / tobe_devs\n",
    "\n",
    "        if explained:\n",
    "            import shap\n",
    "            import matplotlib.pyplot as plt\n",
    "\n",
    "            np.random.seed(42)\n",
    "            e = shap.DeepExplainer(model,\n",
    "                                   torch.FloatTensor(X_train[np.random.choice(X_train.shape[0], 1000, replace=False)]))\n",
    "\n",
    "            shap_idx = []\n",
    "            for j in range(len(y_pred_list)):\n",
    "                if y_pred_list[j][0] == y_test[j][0] == 1:\n",
    "                    shap_idx.append(j)\n",
    "            shap_values = e.shap_values(torch.FloatTensor(X_test[shap_idx]))\n",
    "            fig = shap.summary_plot(shap_values[0], X_test[shap_idx], plot_type='dot', feature_names=enc_dat.columns,\n",
    "                                    max_display=10, plot_size=(10, 5), show=False)\n",
    "            plt.savefig(path + '/ShapValues/Dev_' + z + '_' + d + '.png')\n",
    "            plt.close()\n",
    "\n",
    "            fig = shap.summary_plot(shap_values[1], X_test[shap_idx], plot_type='dot', feature_names=enc_dat.columns,\n",
    "                                    max_display=10, plot_size=(10, 5), show=False)\n",
    "            plt.savefig(path + '/ShapValues/NoDev_' + z + '_' + d + '.png')\n",
    "            plt.close()\n",
    "\n",
    "        print(metrics)\n",
    "\n",
    "    avg_dev_pos = {}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        devs = 0\n",
    "        positions = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] > 0:\n",
    "                devs += 1\n",
    "                positions += dev_position[d][idx]\n",
    "        if devs == 0:\n",
    "            continue\n",
    "        avg_dev_pos[d] = positions / devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df = pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df = pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "\n",
    "    writer.close()\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_CIBE_confidence.xlsx', engine=\"xlsxwriter\")\n",
    "    for i in range(len(y_pred_cum)):\n",
    "        for d in dev_trained:\n",
    "            y_pred_cum[str('confidence' + d)][i] = y_pred_cum[str('confidence' + d)][i][0][0]\n",
    "    confidences = [.5, .6, .7, .8]\n",
    "    for confidence in confidences:\n",
    "        conf_pred = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        auc_conf_pred = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        auc_test = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        for d in dev_trained:\n",
    "            for i in range(len(y_pred_cum)):\n",
    "                if y_pred_cum[str('confidence' + d)][i] >= confidence:\n",
    "                    conf_pred[d][i] = np.array([1.0, 0.0], dtype=float)\n",
    "                else:\n",
    "                    conf_pred[d][i] = np.array([0.0, 1.0], dtype=float)\n",
    "                auc_conf_pred[d][i] = conf_pred[d][i][0]\n",
    "                auc_test[d][i] = y_test_cum[d][i][0]\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(list(y_test_cum[d]), list(conf_pred[d]))\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(list(auc_test[d]), list(auc_conf_pred[d]),\n",
    "                                                                      average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics[str('NoDev' + d)]['Support'] = CM[1][1][1]+CM[1][1][0]\n",
    "        metrics.to_excel(writer, sheet_name=(str('Unfiltered' + str(confidence))))\n",
    "\n",
    "        conf_pred = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        conf_test = pd.DataFrame(columns=dev, index=range(len(y_pred_cum)))\n",
    "        for d in dev_trained:\n",
    "            conf_pred = y_pred_cum[\n",
    "                (y_pred_cum[str('confidence' + d)] > confidence) | (y_pred_cum[str('confidence' + d)] < 1 - confidence)]\n",
    "            conf_test = y_test_cum.loc[conf_pred.index]\n",
    "            auc_test = []\n",
    "            auc_pred = []\n",
    "            for i in conf_pred.index:\n",
    "                auc_test.append(conf_test[d][i][0])\n",
    "                auc_pred.append(conf_pred[d][i][0])\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(list(conf_test[d]), list(conf_pred[d]))\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(auc_test, auc_pred, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "            metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "        metrics.to_excel(writer, sheet_name=(str('Filtered' + str(confidence))))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_separate_CIBE_confidence(log, ref_log, aligned_traces)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def catboost_patterns(log, ref_log, aligned_traces, split=1/3, relevance_ths = .5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][list(max_combs.keys())]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    outputs_train = pd.DataFrame()\n",
    "    outputs_test = pd.DataFrame()\n",
    "    outputs_val = pd.DataFrame()\n",
    "\n",
    "\n",
    "    def flatten_comprehension(matrix):\n",
    "        return [item for row in matrix for item in row]\n",
    "\n",
    "\n",
    "    x_train_idx_c, x_test_idx_c, y_train_idx_c, y_test_idx_c = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                                test_size=split,\n",
    "                                                                                random_state=0)\n",
    "    x_train_idx_c, x_val_idx_c, y_train_idx_c, y_val_idx_c = train_test_split(x_train_idx_c, x_train_idx_c, test_size=0.2,\n",
    "                                                                              random_state=0)\n",
    "\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                      random_state=0)\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        cum_trace_idxs = []\n",
    "        pref_list = []\n",
    "        pref_list_train_c = []\n",
    "        pref_list_test_c = []\n",
    "        pref_list_val_c = []\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_te_c = X_cum[prefix].loc[[j for j in list(set(y_test_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr_c = X_cum[prefix].loc[[j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va_c = X_cum[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            y_tr_c = y_cum_test_o_combs[prefix].loc[\n",
    "                [j for j in list(set(y_train_idx_c) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va_c = y_cum_test_o_combs[prefix].loc[[j for j in list(set(y_val_idx_c) - set(drop_idx))]].to_numpy().astype(\n",
    "                float)\n",
    "            cum_trace_idxs.append(list(set(y_test_idx) - set(drop_idx)))\n",
    "            pref_list.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_train_c.append([prefix] * len(list(set(y_train_idx_c) - set(drop_idx))))\n",
    "            pref_list_test_c.append([prefix] * len(list(set(y_test_idx) - set(drop_idx))))\n",
    "            pref_list_val_c.append([prefix] * len(list(set(y_val_idx_c) - set(drop_idx))))\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "                X_train_c = x_tr_c\n",
    "                X_test_c = x_te_c\n",
    "                X_val_c = x_va_c\n",
    "                y_train_c = y_tr_c\n",
    "                y_test_c = y_te_c\n",
    "                y_val_c = y_va_c\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)\n",
    "                X_train_c = np.append(X_train_c, x_tr_c, axis=0)\n",
    "                X_test_c = np.append(X_test_c, x_te_c, axis=0)\n",
    "                X_val_c = np.append(X_val_c, x_va_c, axis=0)\n",
    "                y_train_c = np.append(y_train_c, y_tr_c, axis=0)\n",
    "                y_test_c = np.append(y_test_c, y_te_c, axis=0)\n",
    "                y_val_c = np.append(y_val_c, y_va_c, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(y_train_c), len(X_val), len(y_val), len(y_val_c), len(X_test), len(y_test),\n",
    "              len(y_test_c))\n",
    "\n",
    "        cat_y_train = y_train[:, 0]\n",
    "        cat_y_val = y_val[:, 0]\n",
    "        cat_y_test = y_test[:, 0]\n",
    "        cat_y_train\n",
    "        from catboost import CatBoostClassifier\n",
    "\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=16, early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(X_train, cat_y_train)\n",
    "            y_pred = catboost.predict(X_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "            y_output_train = catboost.predict_proba(X_train_c)\n",
    "            y_output_test = catboost.predict_proba(X_test_c)\n",
    "            y_output_val = catboost.predict_proba(X_val_c)\n",
    "\n",
    "            outputs_train['NoDev' + str(d)] = y_output_train[:, 0]\n",
    "            outputs_train['Dev' + str(d)] = y_output_train[:, 1]\n",
    "            outputs_test['NoDev' + str(d)] = y_output_test[:, 0]\n",
    "            outputs_test['Dev' + str(d)] = y_output_test[:, 1]\n",
    "            outputs_val['NoDev' + str(d)] = y_output_val[:, 0]\n",
    "            outputs_val['Dev' + str(d)] = y_output_val[:, 1]\n",
    "            if d == dev[0]:\n",
    "                outputs_train['prefix_length'] = flatten_comprehension(pref_list_train_c)\n",
    "                outputs_test['prefix_length'] = flatten_comprehension(pref_list_test_c)\n",
    "                outputs_val['prefix_length'] = flatten_comprehension(pref_list_val_c)\n",
    "\n",
    "        except Exception as er:\n",
    "            metrics[d] = er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()\n",
    "    outputs_test\n",
    "    y_test_c.sum()\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    outputs_train\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for i, label in enumerate(unique_rel_combs):\n",
    "        positives[label] = y_train_c[:, i].sum()\n",
    "        negatives[label] = len(y_train_c) - y_train_c[:, i].sum()\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pir[label] = min((max(positives[label], negatives[label]) / positives[label]), 10000)\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    pw_combs = {}\n",
    "    nw_combs = {}\n",
    "    for label in unique_rel_combs:\n",
    "        pw_combs[label] = min((mean(pir.values()) ** (4 / (2 ** math.e)) + (np.log(pirlbl[label]))), 200)\n",
    "        #pw_combs[label] =  min(2*(mean(pir.values()) ** ((2* math.e)) + (np.log(pirlbl[label]))), 100000)\n",
    "        nw_combs[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "    pw_combs\n",
    "    metrics_comb\n",
    "    if len(y_train_c[0]) == 1:\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=list(pw_combs.values())[0],\n",
    "                                      early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(outputs_train, y_train_c)\n",
    "            y_pred = catboost.predict(outputs_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "            y_test_c = np.column_stack((y_test_c, 1 - y_test_c))\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "            print(CM)\n",
    "            for i, urc in enumerate(unique_rel_combs):\n",
    "                metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "                metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "                metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "                try:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i],\n",
    "                                                                                 np.array(y_pred_list)[:, i],\n",
    "                                                                                 average='macro')\n",
    "                except Exception as er:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = er\n",
    "                metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "                metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "                metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "        except Exception as er:\n",
    "            metrics_comb[unique_rel_combs[0]] = er\n",
    "    else:\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, loss_function='MultiLogloss',\n",
    "                                      early_stopping_rounds=10)\n",
    "        try:\n",
    "            metric_combs = unique_rel_combs\n",
    "            for ri in range(len(y_train_c[0]) - 1, -1, -1):\n",
    "                if y_train_c[:, ri].sum() == 0:\n",
    "                    y_train_c = np.delete(y_train_c, ri, 1)\n",
    "                    y_test_c = np.delete(y_test_c, ri, 1)\n",
    "                    del metric_combs[ri]\n",
    "\n",
    "            catboost.fit(outputs_train, y_train_c)\n",
    "            y_pred = catboost.predict(outputs_test)\n",
    "\n",
    "            y_pred_list = y_pred.tolist()\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test_c, y_pred_list)\n",
    "            print(CM)\n",
    "            for i, urc in enumerate(metric_combs):\n",
    "                metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "                metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "                metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "                try:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test_c[:, i],\n",
    "                                                                                 np.array(y_pred_list)[:, i],\n",
    "                                                                                 average='macro')\n",
    "                except Exception as er:\n",
    "                    metrics_comb[urc]['ROC_AUC'] = er\n",
    "                metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "                metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "                metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "\n",
    "        except Exception as er:\n",
    "            metrics_comb[unique_rel_combs[0]] = er\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_catboost.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## load feature vectors from MPPN\n",
    "file = ask_for_path(REL_INPUT_PATH, 17)  # adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    pd_cases_fv = pickle.load(f)\n",
    "\n",
    "\n",
    "def IDP_separate_MPPN(log, pd_cases_fv, aligned_traces, split=1 / 3, u_sample=True, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i += 1\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "    X_cum = {}\n",
    "    for pref in range(1, max_ev + 1):\n",
    "        X_cum[pref] = pd.DataFrame(columns=['FV'], index=list(range(len(pd_cases_fv['case:concept:name'].unique()))))\n",
    "    no_cases = len(pd_cases_fv['case:concept:name'].unique())\n",
    "    row = 0\n",
    "    for counter in range(len(pd_cases_fv)):\n",
    "        X_cum[len(pd_cases_fv.loc[counter].trace)]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "        if not counter == len(pd_cases_fv) - 1:\n",
    "            if not pd_cases_fv.loc[counter]['case:concept:name'] == pd_cases_fv.loc[counter + 1]['case:concept:name']:\n",
    "                if not len(pd_cases_fv.loc[counter].trace) == max_ev:\n",
    "                    for missing in range(len(pd_cases_fv.loc[counter].trace) + 1, max_ev + 1):\n",
    "                        X_cum[missing]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "                row += 1\n",
    "        else:\n",
    "            if not len(pd_cases_fv.loc[counter].trace) == max_ev:\n",
    "                for missing in range(len(pd_cases_fv.loc[counter].trace) + 1, max_ev + 1):\n",
    "                    X_cum[missing]['FV'][row] = pd_cases_fv.loc[counter].fv\n",
    "            row += 1\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in dev:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_MPPN_Classifier')\n",
    "\n",
    "    writer = pd.ExcelWriter(path + '/' + z + '_BPDP_MPPN_classification.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)),\n",
    "                                                                        test_size=split, random_state=0)\n",
    "\n",
    "    dev_position = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    for d in dev:\n",
    "        for idx in x_test_idx:\n",
    "            for i in range(1, event_count[idx] + 1):\n",
    "                if y_cum_test[i][d][idx] == 1: dev_position[d][idx] = i + 1\n",
    "    dev_position_pred = pd.DataFrame(index=x_test_idx, columns=dev, data=0)\n",
    "    earliness = {}\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.0001\n",
    "\n",
    "    dev_trained = []\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        elif dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        else:\n",
    "            dev_trained.append(d)\n",
    "        if dev_distribution[d]['Test'] / len(x_test_idx) <= 0.05:\n",
    "            metrics[d]['Notes'] = str(\n",
    "                'Only very few deviations in Test Set:' + str(dev_distribution[d]['Test'] / len(x_test_idx)))\n",
    "\n",
    "        elif dev_distribution[d]['Training'] / len(x_train_idx) <= 0.05:\n",
    "            metrics[d]['Notes'] = str(\n",
    "                'Only very few deviations in Training Set:' + str(dev_distribution[d]['Training'] / len(x_train_idx)))\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        if u_sample:\n",
    "            imb_ref_enc_dat = pd.DataFrame(X_cum[1]['FV'].tolist()).add_prefix(\"c\")\n",
    "            imb_ref_enc_dat['ind'] = 0\n",
    "            for i in range(len(imb_ref_enc_dat)):\n",
    "                imb_ref_enc_dat['ind'][i] = i\n",
    "            imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "            for trace in range(len(log)):\n",
    "                if dev_df[d][trace] > 0:\n",
    "                    imb_traces['Dev'][trace] = 1\n",
    "\n",
    "            imb_traces = imb_traces.drop(x_test_idx)\n",
    "            imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "            imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "            imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "\n",
    "            oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "\n",
    "            X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "\n",
    "            x_train_idx = list(X_resampled['ind'])\n",
    "            y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            P = pd.DataFrame(X_cum[prefix]['FV'].tolist()).add_prefix(\"c\")\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = P.loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = P.loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = P.loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "        LEARNING_RATE = 0.0001\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=128)\n",
    "        model.to(device)\n",
    "        weights = torch.FloatTensor(list([positive_weights[d], negative_weights[d]]))\n",
    "        criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if early_stop:\n",
    "\n",
    "            EPOCHS = 300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                   torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch < EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss = 0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1) * len(x_batch)\n",
    "                    epoch_loss += loss\n",
    "                    if i == len(steps) - 1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model, vloss): done = True\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(\n",
    "                            f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "        metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "        metrics[d]['Support'] = CM[0][1][1] + CM[0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = CM[1][1][1] + CM[1][1][0]\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            for idx in to_be_checked_idx.keys():\n",
    "                if event_count[idx] >= prefix:\n",
    "                    if prefix == 1:\n",
    "                        dev_position_pred[d][idx] = y_pred_list[to_be_checked_idx[idx][prefix - 1]][0]\n",
    "                    else:\n",
    "                        if y_pred_list[to_be_checked_idx[idx][prefix - 1]][0] == 1 and \\\n",
    "                                y_pred_list[to_be_checked_idx[idx][prefix - 2]][0] == 0:\n",
    "                            if dev_position[d][idx] <= prefix:\n",
    "                                dev_position_pred[d][idx] = dev_position[d][idx]\n",
    "                            else:\n",
    "                                dev_position_pred[d][idx] = prefix\n",
    "\n",
    "        earliness[d] = 0\n",
    "        tobe_devs = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] == 0 or dev_position_pred[d][idx] == 0:\n",
    "                continue\n",
    "            tobe_devs += 1\n",
    "            earliness[d] += dev_position_pred[d][idx] / dev_position[d][idx]\n",
    "        if not tobe_devs == 0:\n",
    "            earliness[d] = earliness[d] / tobe_devs\n",
    "\n",
    "    avg_dev_pos = {}\n",
    "    for d in dev:\n",
    "        if dev_distribution[d]['Test'] == 0:\n",
    "            metrics[d] = 'No Deviation in Test Set'\n",
    "            continue\n",
    "        if dev_distribution[d]['Training'] == 0:\n",
    "            metrics[d] = 'No Deviation in Training Set'\n",
    "            continue\n",
    "        devs = 0\n",
    "        positions = 0\n",
    "        for idx in to_be_checked_idx.keys():\n",
    "            if dev_position[d][idx] > 0:\n",
    "                devs += 1\n",
    "                positions += dev_position[d][idx]\n",
    "        if devs == 0:\n",
    "            continue\n",
    "        avg_dev_pos[d] = positions / devs\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    df = pd.DataFrame(data=earliness, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Earliness'))\n",
    "    df = pd.DataFrame(data=avg_dev_pos, index=[0])\n",
    "    df.to_excel(writer, sheet_name=('Position'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def genga_benchmark(log, aligned_traces, c=2, alpha=1,split = 1 / 3):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "    #if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "    for j, trace in enumerate(log):\n",
    "        for i, event in enumerate(trace):\n",
    "            pi = i + 1\n",
    "            if pi == 1:\n",
    "                event['duration'] = 0\n",
    "                event['trace'] = event['concept:name']\n",
    "\n",
    "\n",
    "            elif pi <= event_count[j]:\n",
    "                event['duration'] = (log[j][i]['time:timestamp'] - log[j][0]['time:timestamp']).total_seconds() / 60\n",
    "                event['trace'] = str(log[j][i - 1]['trace'] + ', ' + event['concept:name'])\n",
    "        trace.attributes['duration'] = log[j][event_count[j] - 1]['duration']\n",
    "    tree_df = pm4py.convert_to_dataframe(log)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_dataframe = tree_df.filter(items=['case:AMOUNT_REQ', 'duration'], axis=1)\n",
    "        ref_clean_dat = ref_dataframe.rename(columns={'case:AMOUNT_REQ': 'AMOUNT_REQ'})\n",
    "    else:\n",
    "        ref_clean_dat = tree_df.filter(items=['duration'], axis=1)\n",
    "\n",
    "    X_tree = ref_clean_dat.loc[x_train_idx]\n",
    "    for d in dev:\n",
    "        y_tree = dev_df[d][x_train_idx]\n",
    "\n",
    "        from feature_engine.discretisation import DecisionTreeDiscretiser\n",
    "\n",
    "        disc = DecisionTreeDiscretiser(regression=False)\n",
    "\n",
    "        # fit the transformer\n",
    "        disc.fit(X_tree, y_tree)\n",
    "\n",
    "        tree_df = pm4py.convert_to_dataframe(log)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            X_preprosessed = tree_df.filter(items=['case:AMOUNT_REQ', 'duration'], axis=1)\n",
    "            X_preprosessed = X_preprosessed.rename(columns={'case:AMOUNT_REQ': 'AMOUNT_REQ'})\n",
    "        else:\n",
    "            X_preprosessed = tree_df.filter(items=['duration'], axis=1)\n",
    "\n",
    "        try:\n",
    "            X_preprosessed = disc.transform(X_preprosessed)\n",
    "        except Exception as er:\n",
    "            print(er)\n",
    "\n",
    "\n",
    "        genga_states = []\n",
    "        runner = 0\n",
    "        for j, trace in enumerate(log):\n",
    "            for i, event in enumerate(trace):\n",
    "                if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "                    event['genga'] = str(\n",
    "                        event['trace'] + '_' + str(X_preprosessed['AMOUNT_REQ'][runner + i]) + '_' + str(\n",
    "                            X_preprosessed['duration'][runner + i]))\n",
    "                    if not event['genga'] in genga_states:\n",
    "                        genga_states.append(event['genga'])\n",
    "                else:\n",
    "                    event['genga'] = str(\n",
    "                        event['trace'] + '_' + str(\n",
    "                            X_preprosessed['duration'][runner + i]))\n",
    "                    if not event['genga'] in genga_states:\n",
    "                        genga_states.append(event['genga'])\n",
    "            runner += event_count[j]\n",
    "\n",
    "        len(genga_states)\n",
    "        genga_counts = pd.DataFrame(data=0, index=genga_states, columns=[d])\n",
    "        genga_counts['count'] = 0\n",
    "        for j, trace in enumerate(log):\n",
    "            if not j in x_train_idx:\n",
    "                continue\n",
    "            for i, event in enumerate(trace):\n",
    "                genga_counts[d][event['genga']] += dev_df[d][j]\n",
    "                genga_counts['count'][event['genga']] += 1\n",
    "        genga_counts['count'].sum()\n",
    "        test_count = 0\n",
    "        for idx in y_test_idx:\n",
    "            test_count += event_count[idx]\n",
    "        y_true_df = pd.DataFrame(data=0, index=range(test_count), columns=[d])\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max(event_count.values()) + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "\n",
    "        runner = 0\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                y_true_df[d][runner] = y_cum_test[prefix][d][enumerated_trace_idx[prefix][idx]]\n",
    "                runner += 1\n",
    "\n",
    "        genga_states_test = {}\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            genga_states_test[prefix] = []\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                genga_states_test[prefix].append(log[enumerated_trace_idx[prefix][idx]][prefix - 1]['genga'])\n",
    "\n",
    "        y_pred_df = pd.DataFrame(data=0, index=range(test_count), columns=[d])\n",
    "\n",
    "        runner = 0\n",
    "        for prefix in range(1, len(enumerated_trace_idx) + 1):\n",
    "            for idx in range(len(enumerated_trace_idx[prefix])):\n",
    "                p = genga_counts[d][genga_states_test[prefix][idx]]\n",
    "                n = genga_counts['count'][genga_states_test[prefix][idx]] - genga_counts[d][genga_states_test[prefix][idx]]\n",
    "                xb = p / (p + n + c)\n",
    "                xd = n / (p + n + c)\n",
    "                xu = c / (p + n + c)\n",
    "                if (xu < xb or xu < xd) and xb > alpha * xd:\n",
    "                    y_pred_df[d][runner] = 1\n",
    "                runner += 1\n",
    "        y_test_list = y_true_df.values.tolist()\n",
    "        y_pred_list = y_pred_df.values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "        CM = sklearn.metrics.confusion_matrix(y_test_list, y_pred_list)\n",
    "\n",
    "\n",
    "        try:\n",
    "            metrics[d]['Precision'] = CM[1][1] / (CM[1][1] + CM[0][1])\n",
    "            metrics[d]['Recall'] = CM[1][1] / (CM[1][1] + CM[1][0])\n",
    "        except Exception as er:\n",
    "             metrics[d]['Precision']=CM[0]\n",
    "             metrics[d]['Recall']=er\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_true_df[d], y_pred_df[d], average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        try:\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[0][0] / (CM[0][0] + CM[1][0])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[0][0] / (CM[0][0] + CM[0][1])\n",
    "        except Exception as er:\n",
    "            metrics[str('NoDev' + d)]['ROC_AUC'] = er\n",
    "\n",
    "\n",
    "        to_be_checked_idx = {}\n",
    "        for idx in x_test_idx:\n",
    "            cum_idx = 0\n",
    "            for prefix in range(1, event_count[idx] + 1):\n",
    "                if prefix == 1:\n",
    "                    to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                else:\n",
    "                    to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                cum_idx += len(enumerated_trace_idx[prefix])\n",
    "\n",
    "\n",
    "    path = (os.getcwd() + '/Genga')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_Genga_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classify_cat(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    # print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        cat_y_train = y_train[:, 0]\n",
    "        cat_y_val = y_val[:, 0]\n",
    "        cat_y_test = y_test[:, 0]\n",
    "        cat_y_train\n",
    "        from catboost import CatBoostClassifier\n",
    "        catboost = CatBoostClassifier(verbose=False, random_state=0, scale_pos_weight=16, early_stopping_rounds=10)\n",
    "        try:\n",
    "            catboost.fit(X_train, cat_y_train)\n",
    "            y_pred = catboost.predict(X_test)\n",
    "            y_pred\n",
    "            y_preds = np.stack((y_pred, 1 - y_pred), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "        except Exception as er:\n",
    "            metrics[d]=er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def classify_xgb(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        k += 1\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "\n",
    "    # print(len(dev), dev)\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe = ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe = ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe = ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe = ref_dataframe.reset_index()\n",
    "    ref_raw_dat = ref_dataframe.drop('index', axis=1)\n",
    "    if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ'] = pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat = ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z == 'aligned_traces_20int.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(\n",
    "            ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID', 'Permit id'],\n",
    "            axis=1)\n",
    "    elif z == 'aligned_traces_20dom.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "    elif z == 'aligned_traces_20prep.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "    elif z == 'aligned_traces_20RfP.pkl':\n",
    "        ref_clean_dat = ref_raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat = ref_raw_dat.copy()\n",
    "    ref_enc_dat = pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum = {}\n",
    "    metrics = pd.DataFrame(data=0, columns=dev,\n",
    "                           index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        complex_index_encoding(log, prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe = dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe = dataframe.filter(like='case:', axis=1)\n",
    "        dataframe = dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe = dataframe.reset_index()\n",
    "        raw_dat = dataframe.drop('index', axis=1)\n",
    "        if z == 'aligned_traces_12A.pkl' or z == 'aligned_traces_12O.pkl' or z == 'aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ'] = pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat = raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z == 'aligned_traces_20int.pkl':\n",
    "            clean_dat = raw_dat.drop(\n",
    "                ['Permit travel permit number', 'DeclarationNumber', 'travel permit number', 'id', 'Permit ID',\n",
    "                 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20dom.pkl':\n",
    "            clean_dat = raw_dat.drop(['DeclarationNumber', 'id'], axis=1)\n",
    "        elif z == 'aligned_traces_20prep.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id', 'Permit travel permit number', 'Permit id'], axis=1)\n",
    "        elif z == 'aligned_traces_20RfP.pkl':\n",
    "            clean_dat = raw_dat.drop(['RfpNumber', 'Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat = raw_dat.copy()\n",
    "        enc_dat = pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key] = 0  # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        enc_dat = pd.DataFrame(data=imp.fit_transform(enc_dat), columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix] = enc_dat.copy()\n",
    "        drop_idx = []\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx] < prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix] = X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = 16\n",
    "        negative_weights[label] = 1\n",
    "        #positive_weights[label] = (mean(pir.values())+statistics.stdev(pir.values()))**(1/(2*math.e))+np.log(pir[label])\n",
    "        #negative_weights[label] = (mean(nir.values())+statistics.stdev(nir.values()))**(1/(2*math.e))+np.log(nir[label])\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "\n",
    "    path = (os.getcwd() + '/CatBoost')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MC_catoost_ES_' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "    dev_distribution = pd.DataFrame(data=0, index=['Training', 'Test'], columns=dev)\n",
    "\n",
    "    for d in dev:\n",
    "        dev_distribution[d]['Training'] = sum(dev_df[d][i] for i in x_train_idx)\n",
    "        dev_distribution[d]['Test'] = sum(dev_df[d][i] for i in x_test_idx)\n",
    "\n",
    "    dev_distribution.to_excel(writer, sheet_name=('Distribution'))\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "        Y_cum_dev = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            Y_cum_dev[prefix] = pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev'] = 0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i] = 1 - y_cum_test[prefix][d][i]\n",
    "            if prefix == 1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "        print('index length ', len(x_train_idx), len(x_test_idx), len(y_train_idx), len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2,\n",
    "                                                                          random_state=0)\n",
    "\n",
    "        enumerated_trace_idx = {}\n",
    "        for prefix in range(1, max_ev + 1):\n",
    "            drop_idx = []\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx] < prefix:\n",
    "                    drop_idx.append(trace_idx)  # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te = X_cum[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr = X_cum[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va = X_cum[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te = Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr = Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va = Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix] = list(set(y_test_idx) - set(drop_idx))\n",
    "            print('subset length ', prefix, len(x_te), len(x_tr), len(y_te), len(y_tr))\n",
    "\n",
    "            if prefix == 1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append(X_train, x_tr, axis=0)\n",
    "                X_test = np.append(X_test, x_te, axis=0)\n",
    "                y_train = np.append(y_train, y_tr, axis=0)\n",
    "                y_test = np.append(y_test, y_te, axis=0)\n",
    "                y_val = np.append(y_val, y_va, axis=0)\n",
    "                X_val = np.append(X_val, x_va, axis=0)  # combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train), len(y_train), len(X_val), len(y_val), len(X_test), len(y_test))\n",
    "\n",
    "        cxgb_y_train = y_train[:, 0]\n",
    "        xgb_y_val = y_val[:, 0]\n",
    "        xgb_y_test = y_test[:, 0]\n",
    "        xgb_y_train\n",
    "        bst = xgb.XGBClassifier(max_depth=16, scale_pos_weight=16)\n",
    "\n",
    "        try:\n",
    "            bst.fit(X_train, xgb_y_train, eval_set=[(X_train, xgb_y_train), (X_val, xgb_y_val)], early_stopping_rounds=10)\n",
    "            y_preds = bst.predict(X_test)\n",
    "            y_preds\n",
    "            y_preds = np.stack((y_preds, 1 - y_preds), axis=1)\n",
    "            y_pred_list = y_preds.tolist()\n",
    "            y_pred_list\n",
    "\n",
    "            CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "            metrics[d]['Precision'] = CM[0][1][1] / (CM[0][1][1] + CM[0][0][1])\n",
    "            metrics[d]['Recall'] = CM[0][1][1] / (CM[0][1][1] + CM[0][1][0])\n",
    "            try:\n",
    "                metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "            except Exception as er:\n",
    "                metrics[d]['ROC_AUC'] = er\n",
    "            metrics[str('NoDev' + d)]['Precision'] = CM[1][1][1] / (CM[1][1][1] + CM[1][0][1])\n",
    "            metrics[str('NoDev' + d)]['Recall'] = CM[1][1][1] / (CM[1][1][1] + CM[1][1][0])\n",
    "\n",
    "            print(CM)\n",
    "\n",
    "            to_be_checked_idx = {}\n",
    "            for idx in x_test_idx:\n",
    "                cum_idx = 0\n",
    "                for prefix in range(1, event_count[idx] + 1):\n",
    "                    if prefix == 1:\n",
    "                        to_be_checked_idx[idx] = [enumerated_trace_idx[1].index(idx)]\n",
    "                    else:\n",
    "                        to_be_checked_idx[idx].append(enumerated_trace_idx[prefix].index(idx) + cum_idx)\n",
    "                    cum_idx += len(enumerated_trace_idx[prefix])\n",
    "        except Exception as er:\n",
    "            metrics[d]=er\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Suffix Prediction: Load Suffixes and execute prediction of deviations\n",
    "file= ask_for_path(REL_INPUT_PATH,25)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    suffixes=pickle.load(f)\n",
    "def suffix_prediction_deviations(log, aligned_traces, net, initial_marking, final_marking, suffixes,split = 1 / 3):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    if z == 'MPPN_BPIC_2020_request_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_international_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_prepaid_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_domestic_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf]=str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    suffixes = suffixes.rename(columns={'case:concept:name': 'case:trace_ID', 'IDX': 'case:IDX'})\n",
    "    suffixes['case:concept:name'] = suffixes[[\"case:trace_ID\", \"case:IDX\"]].astype(str).apply(\"_\".join, axis=1)\n",
    "\n",
    "    suffixes['case:prefix'] = 1\n",
    "    for suf in range(1, len(suffixes)):\n",
    "        if suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and suffixes['case:IDX'][suf - 1] == \\\n",
    "                suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1]\n",
    "        elif suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and not suffixes['case:IDX'][suf - 1] == \\\n",
    "                                                                                          suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1] + 1\n",
    "\n",
    "    events_per_trace = pd.DataFrame(data=0, columns=['count'], index=suffixes['case:IDX'].unique())\n",
    "    for idx in range(len(suffixes)):\n",
    "        events_per_trace['count'][suffixes['case:IDX'][idx]] += 1\n",
    "\n",
    "    predicted_log = pm4py.convert_to_event_log(suffixes)\n",
    "\n",
    "\n",
    "    aligned_predictions = pm4py.conformance_diagnostics_alignments(predicted_log, net, initial_marking, final_marking)\n",
    "\n",
    "    i = 0\n",
    "    pred_dev = []\n",
    "    for trace in predicted_log:\n",
    "        no_moves = len(aligned_predictions[i]['alignment'])\n",
    "        for j in range(0, len(aligned_predictions[i]['alignment'])):\n",
    "            if aligned_predictions[i]['alignment'][j][1] == None or aligned_predictions[i]['alignment'][j][0] == \\\n",
    "                    aligned_predictions[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_predictions[i]['alignment'][j]) in pred_dev:\n",
    "                    pred_dev.append(str(aligned_predictions[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    print(dev)\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    dev_df['trace_ID'] = 0\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        dev_df['trace_ID'][k] = trace.attributes['concept:name']\n",
    "        k += 1\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    caseIDs_test = []\n",
    "    key_caseID_testIDX = {}\n",
    "    for idx in x_test_idx:\n",
    "        caseIDs_test.append(log[idx].attributes['concept:name'])\n",
    "        key_caseID_testIDX[log[idx].attributes['concept:name']] = idx\n",
    "    caseIDs_train = []\n",
    "    for idx in x_train_idx:\n",
    "        caseIDs_train.append(log[idx].attributes['concept:name'])\n",
    "    y_cum_pred = {}\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_pred[\n",
    "            ev] = pd.DataFrame(data=0, columns=dev, index=caseIDs_test)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].set_index('trace_ID')\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "    for i, alignment in enumerate(aligned_predictions):\n",
    "        no_moves = len(alignment['alignment'])\n",
    "        for d in dev:\n",
    "            for j in range(no_moves):\n",
    "                if str(aligned_predictions[i]['alignment'][j]) == d:\n",
    "                    #print(i, predicted_log[i].attributes['trace_ID'],d, predicted_log[i].attributes['prefix'])\n",
    "                    y_cum_pred[predicted_log[i].attributes['prefix']][d][predicted_log[i].attributes['trace_ID']] = 1\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[ev] = y_cum_test[ev].sort_index()\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].sort_index()\n",
    "\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred[ev]\n",
    "            y_test = y_cum_test[ev]\n",
    "        else:\n",
    "\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test[ev], axis=0)\n",
    "    len(y_cum_test[1].values.tolist())\n",
    "    print(len(y_pred_list))\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                       average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "    path = (os.getcwd() + '/Suffix_Prediction')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_suffix_pred_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "file= ask_for_path(REL_INPUT_PATH,14)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    suffixes=pickle.load(f)\n",
    "def suffix_prediction_patterns(log, aligned_traces, net, initial_marking, final_marking, suffixes,split = 1 / 3, relevance_ths = 0.5):\n",
    "    xt, z = os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    if z == 'MPPN_BPIC_2020_request_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_international_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_prepaid_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('request for payment ' + str(suffixes['case:concept:name'][suf]))\n",
    "    elif z == 'MPPN_BPIC_2020_domestic_deviation_detection_suffixe.pkl':\n",
    "        for suf in range(len(suffixes)):\n",
    "            suffixes['case:concept:name'][suf] = str('declaration ' + str(suffixes['case:concept:name'][suf]))\n",
    "    suffixes = suffixes.rename(columns={'case:concept:name': 'case:trace_ID', 'IDX': 'case:IDX'})\n",
    "    suffixes['case:concept:name'] = suffixes[[\"case:trace_ID\", \"case:IDX\"]].astype(str).apply(\"_\".join, axis=1)\n",
    "\n",
    "    suffixes['case:prefix'] = 1\n",
    "    for suf in range(1, len(suffixes)):\n",
    "        if suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and suffixes['case:IDX'][suf - 1] == \\\n",
    "                suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1]\n",
    "        elif suffixes['case:trace_ID'][suf - 1] == suffixes['case:trace_ID'][suf] and not suffixes['case:IDX'][suf - 1] == \\\n",
    "                                                                                          suffixes['case:IDX'][suf]:\n",
    "            suffixes['case:prefix'][suf] = suffixes['case:prefix'][suf - 1] + 1\n",
    "\n",
    "    events_per_trace = pd.DataFrame(data=0, columns=['count'], index=suffixes['case:IDX'].unique())\n",
    "    for idx in range(len(suffixes)):\n",
    "        events_per_trace['count'][suffixes['case:IDX'][idx]] += 1\n",
    "\n",
    "    predicted_log = pm4py.convert_to_event_log(suffixes)\n",
    "\n",
    "    aligned_predictions = pm4py.conformance_diagnostics_alignments(predicted_log, net, initial_marking, final_marking)\n",
    "\n",
    "    i = 0\n",
    "    pred_dev = []\n",
    "    for trace in predicted_log:\n",
    "        no_moves = len(aligned_predictions[i]['alignment'])\n",
    "        for j in range(0, len(aligned_predictions[i]['alignment'])):\n",
    "            if aligned_predictions[i]['alignment'][j][1] == None or aligned_predictions[i]['alignment'][j][0] == \\\n",
    "                    aligned_predictions[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                if not str(aligned_predictions[i]['alignment'][j]) in pred_dev:\n",
    "                    pred_dev.append(str(aligned_predictions[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "    i = 0\n",
    "    dev = []  # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next  # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i += 1\n",
    "\n",
    "    dev\n",
    "\n",
    "    y_cum_test = {}  # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df = pd.DataFrame(data=0, columns=dev, index=range(\n",
    "        len(log)))  # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    dev_df['trace_ID'] = 0\n",
    "    event_order = {}  # dict with event sequences for each trace\n",
    "    event_count = {}  # dict with trace length for each trace\n",
    "    max_ev = 0  # will be maximum trace length\n",
    "    k = 0\n",
    "    for trace in log:\n",
    "        event_order[k] = []\n",
    "        i = 0\n",
    "        for event in trace:\n",
    "            i += 1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i > max_ev:\n",
    "            max_ev = i\n",
    "        event_count[k] = len(event_order[k])\n",
    "        dev_df['trace_ID'][k] = trace.attributes['concept:name']\n",
    "        k += 1\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0, len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0] == \\\n",
    "                    aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i] = 1\n",
    "        i += 1\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = dev_df.copy()  # initialize each prefix length with all traces and information whether deviation happened\n",
    "\n",
    "    i = 0\n",
    "    for trace in log:\n",
    "        no_moves = len(aligned_traces[i]['alignment'])\n",
    "        j = no_moves - 1  # iterator over moves in alignment, starting at the end\n",
    "        m = len(event_order[i])  # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >= 0:\n",
    "            if aligned_traces[i]['alignment'][j][\n",
    "                1] == None:  # if silent move, just go one move further to the beginning in the alignment\n",
    "                j -= 1\n",
    "            elif aligned_traces[i]['alignment'][j][0] == aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m - 1] == aligned_traces[i]['alignment'][j][\n",
    "                    0]:  # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j -= 1\n",
    "                    m -= 1\n",
    "            elif event_order[i][m - 1] == aligned_traces[i]['alignment'][j][0]:  # log move detected\n",
    "                for q in range(m, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j -= 1\n",
    "                m -= 1\n",
    "            elif m == max_ev:\n",
    "                j -= 1\n",
    "            else:  # model move deteceted\n",
    "                for q in range(m + 1, max_ev + 1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][\n",
    "                        i] = 0  # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j -= 1\n",
    "        i += 1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    trainin_dev_df = dev_df.loc[x_train_idx]\n",
    "    trainin_dev_df\n",
    "    trainin_dev_df.corr()\n",
    "    corrMatrix = trainin_dev_df.corr()\n",
    "\n",
    "    corrMatrix.loc[:, :] = np.tril(corrMatrix, k=-1)  # borrowed from Karl D's answer\n",
    "\n",
    "    already_in = set()\n",
    "    max_combs_l = []\n",
    "    for col in corrMatrix:\n",
    "        perfect_corr = corrMatrix[col][corrMatrix[col] >= relevance_ths].index.tolist()\n",
    "        if perfect_corr and col not in already_in:\n",
    "            already_in.update(set(perfect_corr))\n",
    "            perfect_corr.append(col)\n",
    "            max_combs_l.append(perfect_corr)\n",
    "    max_combs_l\n",
    "    test_counts = {}\n",
    "    for comb in max_combs_l:\n",
    "        for y in range(len(comb)):\n",
    "            test_counts[comb[y]] = dev_df.loc[x_test_idx].sum()[comb[y]]\n",
    "        if any(dev_df.loc[x_test_idx].sum()[comb[y]] == 0 for y in range(len(comb))):\n",
    "            max_combs_l.remove(comb)\n",
    "            print(comb)\n",
    "    max_combs_l\n",
    "    test_counts\n",
    "    max_combs = {}\n",
    "    for comb in max_combs_l:\n",
    "        max_combs[str(comb)] = comb\n",
    "    max_combs\n",
    "    y_cum_test[1]\n",
    "    y_cum_test_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_combs[prefix] = y_cum_test[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_test_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_test_combs[prefix].index):\n",
    "                if event_count[i] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_test_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_test_combs[prefix][j][i] = 0\n",
    "                    y_cum_test_combs[prefix][comb][i] = 1\n",
    "    trainin_dev_df.sum()\n",
    "    dev_df.loc[x_test_idx].sum()[max_combs_l[0][0]]\n",
    "    pi = 4\n",
    "    print(y_cum_test_combs[pi].sum())\n",
    "    print(y_cum_test[pi].sum())\n",
    "    y_cum_test_o_combs = {}\n",
    "    columns_needed = list(max_combs.keys())\n",
    "    columns_needed.append('trace_ID')\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[prefix] = y_cum_test_combs[prefix][columns_needed]\n",
    "    y_cum_test_o_combs[1].sum()\n",
    "    y_cum_test_combs[1]\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split,\n",
    "                                                                        random_state=0)\n",
    "    caseIDs_test = []\n",
    "    key_caseID_testIDX = {}\n",
    "    for idx in x_test_idx:\n",
    "        caseIDs_test.append(log[idx].attributes['concept:name'])\n",
    "        key_caseID_testIDX[log[idx].attributes['concept:name']] = idx\n",
    "    caseIDs_train = []\n",
    "    for idx in x_train_idx:\n",
    "        caseIDs_train.append(log[idx].attributes['concept:name'])\n",
    "    y_cum_pred = {}\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_pred[\n",
    "            ev] = pd.DataFrame(data=0, columns=dev, index=caseIDs_test)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].set_index('trace_ID')\n",
    "        y_cum_test_o_combs[ev] = y_cum_test_o_combs[ev].set_index('trace_ID')\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[\n",
    "            ev] = y_cum_test[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "        y_cum_test_o_combs[\n",
    "            ev] = y_cum_test_o_combs[\n",
    "            ev].drop(index=caseIDs_train)\n",
    "    for i, alignment in enumerate(aligned_predictions):\n",
    "        no_moves = len(alignment['alignment'])\n",
    "        for d in dev:\n",
    "            for j in range(no_moves):\n",
    "                if str(aligned_predictions[i]['alignment'][j]) == d:\n",
    "                    #print(i, predicted_log[i].attributes['trace_ID'],d, predicted_log[i].attributes['prefix'])\n",
    "                    y_cum_pred[predicted_log[i].attributes['prefix']][d][predicted_log[i].attributes['trace_ID']] = 1\n",
    "\n",
    "    y_cum_pred_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_pred_combs[prefix] = y_cum_pred[prefix].copy(deep=True)\n",
    "        for comb in max_combs.keys():\n",
    "            y_cum_pred_combs[prefix][comb] = 0\n",
    "            for i in list(y_cum_pred_combs[prefix].index):\n",
    "                if event_count[key_caseID_testIDX[i]] < prefix:\n",
    "                    continue\n",
    "                if all(y_cum_pred_combs[prefix][j][i] == 1 for j in max_combs[comb]):\n",
    "                    for j in max_combs[comb]:\n",
    "                        y_cum_pred_combs[prefix][j][i] = 0\n",
    "                    y_cum_pred_combs[prefix][comb][i] = 1\n",
    "    y_cum_pred_combs\n",
    "    y_cum_pred_o_combs = {}\n",
    "    for prefix in range(1, max_ev + 1):\n",
    "        y_cum_pred_o_combs[prefix] = y_cum_pred_combs[prefix][list(max_combs.keys())]\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev] = y_cum_test[ev].drop(drop_idx)\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test[ev] = y_cum_test[ev].sort_index()\n",
    "        y_cum_pred[ev] = y_cum_pred[ev].sort_index()\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred[ev]\n",
    "            y_test = y_cum_test[ev]\n",
    "        else:\n",
    "\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test[ev], axis=0)\n",
    "    len(y_cum_test[1].values.tolist())\n",
    "    print(len(y_pred_list))\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    metrics = pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                       average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "    path = (os.getcwd() + '/Suffix_Prediction')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_suffix_pred_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()\n",
    "    len(y_pred_list)\n",
    "    metrics\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        drop_idx = []\n",
    "        for trace_idx in caseIDs_test:\n",
    "            if event_count[key_caseID_testIDX[trace_idx]] < ev:\n",
    "                drop_idx.append(trace_idx)  # drop all trace labels that do not go until prefix length\n",
    "        y_cum_pred_o_combs[ev] = y_cum_pred_o_combs[ev].drop(drop_idx)\n",
    "        y_cum_test_o_combs[ev] = y_cum_test_o_combs[ev].drop(drop_idx)\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        y_cum_test_o_combs[ev] = y_cum_test_o_combs[ev].sort_index()\n",
    "        y_cum_pred_o_combs[ev] = y_cum_pred_o_combs[ev].sort_index()\n",
    "\n",
    "    for ev in range(1, max_ev + 1):\n",
    "        if ev == 1:\n",
    "            y_pred_list = y_cum_pred_o_combs[ev]\n",
    "            y_test = y_cum_test_o_combs[ev]\n",
    "        else:\n",
    "            y_pred_list = np.append(y_pred_list, y_cum_pred_o_combs[ev], axis=0)\n",
    "            y_test = np.append(y_test, y_cum_test_o_combs[ev], axis=0)\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "    print(CM)\n",
    "\n",
    "    unique_rel_combs = list(max_combs.keys())\n",
    "    metrics_comb = pd.DataFrame(data=0, columns=unique_rel_combs, index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for d in unique_rel_combs:\n",
    "        metrics_comb[str('NoDev' + d)] = 0\n",
    "    metrics_comb\n",
    "\n",
    "    for i, urc in enumerate(unique_rel_combs):\n",
    "        metrics_comb[urc]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics_comb[urc]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        metrics_comb[urc]['Support'] = (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics_comb[urc]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:, i], np.array(y_pred_list)[:, i],\n",
    "                                                                         average='macro')\n",
    "        except Exception as er:\n",
    "            metrics_comb[urc]['ROC_AUC'] = er\n",
    "        metrics_comb[str('NoDev' + urc)]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics_comb[str('NoDev' + urc)]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "        metrics_comb[str('NoDev' + urc)]['Support'] = CM[i][0][0] + CM[i][0][1]\n",
    "    writer = pd.ExcelWriter('BPDP_combinations/' + z + '_suffix.xlsx', engine=\"xlsxwriter\")\n",
    "    metrics_comb.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "suffix_prediction_patterns(log, aligned_traces, net, initial_marking, final_marking, suffixes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "REL_INPUT_PATH = \"/../mppn_cms/\" # here lie the event logs (.csv), the to-be model (.bpmn) and the already aligned traces (.pkl)\n",
    "file= ask_for_path(REL_INPUT_PATH,7)# adjust to your path\n",
    "with open(file, 'rb') as f:\n",
    "    cm=pickle.load(f)\n",
    "def get_metrics_IDP_MPPN(cm):\n",
    "    metrics = pd.DataFrame(data=0, columns=list(cm.keys()), index=['Precision', 'Recall', 'Support', 'ROC_AUC'])\n",
    "    for i, d in enumerate(list(cm.keys())):\n",
    "        metrics[str('NoDev' + d)] = 0\n",
    "        metrics[str('NoDev' + d)]['Precision'] = cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][0][0][1])\n",
    "        metrics[str('NoDev' + d)]['Recall'] = cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][0][1][0])\n",
    "        metrics[str('NoDev' + d)]['Support'] = cm[list(cm.keys())[i]]['conf_matrix'][0][1][1] + \\\n",
    "                                cm[list(cm.keys())[i]]['conf_matrix'][0][1][0]\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] = sklearn.metrics.roc_auc_score(cm[list(cm.keys())[i]]['targets'],\n",
    "                                                                  cm[list(cm.keys())[i]]['predictions'], average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[d]['Precision'] = cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][1][0][1])\n",
    "        metrics[d]['Recall'] = cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] / (\n",
    "                    cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] + cm[list(cm.keys())[i]]['conf_matrix'][1][1][0])\n",
    "        metrics[d]['Support'] = cm[list(cm.keys())[i]]['conf_matrix'][1][1][1] + \\\n",
    "                                               cm[list(cm.keys())[i]]['conf_matrix'][1][1][0]\n",
    "    metrics\n",
    "    path = (os.getcwd() + '/MPPN2End')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_MPPN2End.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()\n",
    "get_metrics_IDP_MPPN(cm)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm[list(cm.keys())[3]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cm[list(cm.keys())[3]]['conf_matrix'][1][1][1] / (\n",
    "                    cm[list(cm.keys())[3]]['conf_matrix'][1][1][1] + cm[list(cm.keys())[0]]['conf_matrix'][1][1][0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "##### Here start the comparisons to other design choices"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_no_imbalance(log, ref_log, aligned_traces, split=1/3, early_stop=True):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    # writer = pd.ExcelWriter(path+'/'+z+'_Prediction Evaluation CIBE.xlsx', engine=\"xlsxwriter\") # name your excel file\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "                #trace.attributes[str(aligned_traces[i]['alignment'][j])]=1\n",
    "        i+=1\n",
    "    print(len(dev), dev)\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that  stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "\n",
    "\n",
    "    print(dev_df.sum())\n",
    "\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchorous move, just go one move further to the beginning in the alignment and one vent forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000)\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    min_pref=1\n",
    "    max_pref=max_ev\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE=0.0001\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'ROC_AUC'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "\n",
    "    import math\n",
    "\n",
    "\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "\n",
    "    path=(os.getcwd()+'/BPDP_Classifier')\n",
    "    xt,z =os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(path+'/'+z+'_MC_CrossEntropyLoss_NoImbalance_'+str(early_stop)+'_'+str(round(split,2))+'.xlsx', engine=\"xlsxwriter\")\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for d in dev:\n",
    "\n",
    "\n",
    "        Y_cum_dev={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            Y_cum_dev[prefix]=pd.DataFrame(y_cum_test[prefix][d])\n",
    "            Y_cum_dev[prefix]['NoDev']=0\n",
    "            for i in Y_cum_dev[prefix].index.values.tolist():\n",
    "                Y_cum_dev[prefix]['NoDev'][i]=1-y_cum_test[prefix][d][i]\n",
    "            if prefix==1:\n",
    "                print(Y_cum_dev[prefix].columns)\n",
    "\n",
    "\n",
    "        x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "        print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "\n",
    "        x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "        enumerated_trace_idx={}\n",
    "        for prefix in range(1,max_ev+1):\n",
    "            drop_idx=[]\n",
    "            for trace_idx in range(len(log)):\n",
    "                if event_count[trace_idx]< prefix:\n",
    "                    drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "            x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_te=Y_cum_dev[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_tr=Y_cum_dev[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            y_va=Y_cum_dev[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "            enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "            print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "            if prefix ==1:\n",
    "                X_train = x_tr\n",
    "                X_test = x_te\n",
    "                y_train = y_tr\n",
    "                y_test = y_te\n",
    "                X_val = x_va\n",
    "                y_val = y_va\n",
    "            else:\n",
    "                X_train = np.append( X_train, x_tr, axis=0)\n",
    "                X_test = np.append( X_test, x_te, axis=0)\n",
    "                y_train = np.append( y_train, y_tr, axis=0)\n",
    "                y_test = np.append( y_test, y_te, axis=0)\n",
    "                y_val = np.append( y_val, y_va, axis=0)\n",
    "                X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "        print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "\n",
    "        print('split done')\n",
    "        scaler = StandardScaler()\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "        device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        model = BinaryClassificationIndiv(no_columns=len(ref_enc_dat.loc[0]))\n",
    "        model.to(device)\n",
    "\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        if not early_stop:\n",
    "\n",
    "            print(dev, 'training start no ES')\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            for e in range(1, EPOCHS+1):\n",
    "                epoch_loss = 0\n",
    "                epoch_acc = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    y_pred = model(X_batch)\n",
    "\n",
    "                    loss = criterion(y_pred.unsqueeze(1), y_batch.unsqueeze(1))\n",
    "                    acc = binary_acc(y_pred.unsqueeze(1), y_batch.unsqueeze(1))/len(y_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "                print(f'Epoch {e+0:03}: | Loss: {epoch_loss/len(train_loader):.5f} | Acc: {epoch_acc/len(train_loader):.3f} |', d)\n",
    "\n",
    "        else:\n",
    "            print('training start with ES')\n",
    "            EPOCHS=300\n",
    "            model.train()\n",
    "            train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                                torch.FloatTensor(y_train))\n",
    "\n",
    "            train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "            X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "            es = EarlyStopping()\n",
    "            done = False\n",
    "\n",
    "            epoch = 0\n",
    "            while epoch<EPOCHS and not done:\n",
    "                epoch += 1\n",
    "                steps = list(enumerate(train_loader))\n",
    "                pbar = tqdm.tqdm(steps)\n",
    "                model.train()\n",
    "                epoch_acc = 0\n",
    "                epoch_loss=0\n",
    "                for i, (x_batch, y_batch) in pbar:\n",
    "                    optimizer.zero_grad()\n",
    "                    y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                    loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                    acc = binary_acc(y_batch_pred, y_batch.to(device))/len(y_batch_pred[0])\n",
    "\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    epoch_acc += acc.item()\n",
    "\n",
    "                    loss, current = loss.item(), (i + 1)* len(x_batch)\n",
    "                    epoch_loss+=loss\n",
    "                    if i == len(steps)-1:\n",
    "                        model.eval()\n",
    "                        pred = model(X_val)\n",
    "                        vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                        if es(model,vloss): done = True\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss: {epoch_loss/len(train_loader)}, Acc: {epoch_acc/len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                    else:\n",
    "                        pbar.set_description(f\"Epoch: {epoch}, tloss {epoch_loss/len(train_loader):}, Acc: {epoch_acc/len(train_loader):.3f}\")\n",
    "\n",
    "        model.eval()\n",
    "        test_data = TestData(torch.FloatTensor(X_test))\n",
    "        test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "        y_pred_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                y_test_pred = torch.nn.functional.softmax(model(X_batch))\n",
    "                #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "\n",
    "                y_pred_tag = torch.round(y_test_pred)\n",
    "                y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "        y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "        CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "        metrics[d]['Precision']=CM[0][1][1]/(CM[0][1][1]+CM[0][0][1])\n",
    "        metrics[d]['Recall']=CM[0][1][1]/(CM[0][1][1]+CM[0][1][0])\n",
    "        try:\n",
    "            metrics[d]['ROC_AUC'] =  sklearn.metrics.roc_auc_score(y_test, y_pred_list, average='weighted')\n",
    "        except Exception as er:\n",
    "            metrics[d]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev'+d)]['Precision']=CM[1][1][1]/(CM[1][1][1]+CM[1][0][1])\n",
    "        metrics[str('NoDev'+d)]['Recall']=CM[1][1][1]/(CM[1][1][1]+CM[1][1][0])\n",
    "        print(CM)\n",
    "\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we define our FFN\n",
    "class BinaryClassification(nn.Module):\n",
    "    def __init__(self, no_columns, no_devs):\n",
    "        super(BinaryClassification, self).__init__()\n",
    "        # Number of input features is 12.\n",
    "        self.layer_1 = nn.Linear(no_columns, 2048)\n",
    "        self.activation1 = nn.LeakyReLU()\n",
    "        self.layer_2 = nn.Linear(2048, 2048)\n",
    "        self.activation2 = nn.LeakyReLU()\n",
    "        self.layer_3 = nn.Linear(2048, 1024)\n",
    "        self.activation3 = nn.LeakyReLU()\n",
    "        self.layer_out = nn.Linear(1024, no_devs)\n",
    "\n",
    "\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.batchnorm1 = nn.LayerNorm(2048)\n",
    "        self.batchnorm2 = nn.LayerNorm(1024)\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = self.activation1(self.layer_1(inputs))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation2(self.layer_2(x))\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.activation3(self.layer_3(x))\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.layer_out(x)\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def IDP_collective_CIBE(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True):\n",
    "    xt,z =os.path.split(file)\n",
    "    import warnings\n",
    "    warnings.simplefilter('ignore')\n",
    "    #### get information whether deviation happened after prefix length in DF\n",
    "    i=0\n",
    "    dev=[] # stores all deviations that happened\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next # we do not care for simultaneous or silent moves\n",
    "            else:\n",
    "                if not str(aligned_traces[i]['alignment'][j]) in dev:\n",
    "                    dev.append(str(aligned_traces[i]['alignment'][j]))\n",
    "        i+=1\n",
    "\n",
    "    y_cum_test={} # dict that stores label for each prefix and deviation combinations; keys are prefix length, entries are Data Frames with index = trace and columns = deviation\n",
    "    dev_df=pd.DataFrame(data=0,columns=dev, index=range(len(log))) # Data Frame that stores the information whether a deviation happened for each trace on trace level\n",
    "    event_order={} # dict with event sequences for each trace\n",
    "    event_count={} # dict with trace length for each trace\n",
    "    max_ev=0 # will be maximum trace length\n",
    "    k=0\n",
    "    for trace in log:\n",
    "        event_order[k]=[]\n",
    "        i=0\n",
    "        for event in trace:\n",
    "            i+=1\n",
    "            event_order[k].append(event['concept:name'])\n",
    "        if i>max_ev:\n",
    "            max_ev=i\n",
    "        event_count[k]=len(event_order[k])\n",
    "        k+=1\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        for j in range(0,len(aligned_traces[i]['alignment'])):\n",
    "            if aligned_traces[i]['alignment'][j][1] == None or aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                next\n",
    "            else:\n",
    "                dev_df[str(aligned_traces[i]['alignment'][j])][i]=1\n",
    "        i+=1\n",
    "    for ev in range(1,max_ev+1):\n",
    "        y_cum_test[ev]=dev_df.copy() # initialize each prefix length with all traces and information whether deviation happened\n",
    "    for ev in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< ev:\n",
    "                drop_idx.append(trace_idx) # drop all trace labels that do not go until prefix length\n",
    "        y_cum_test[ev]=y_cum_test[ev].drop(drop_idx)\n",
    "    i=0\n",
    "    for trace in log:\n",
    "        no_moves=len(aligned_traces[i]['alignment'])\n",
    "        j=no_moves-1 # iterator over moves in alignment, starting at the end\n",
    "        m=len(event_order[i]) # iterator over event sequence, starting at the end\n",
    "        while j >=0:\n",
    "            if aligned_traces[i]['alignment'][j][1] == None: # if silent move, just go one move further to the beginning in the alignment\n",
    "                j-=1\n",
    "            elif aligned_traces[i]['alignment'][j][0]==aligned_traces[i]['alignment'][j][1]:\n",
    "                if event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # if synchronous move, just go one move further to the beginning in the alignment and one event forther to the beginning in the event sequence\n",
    "                    j-=1\n",
    "                    m-=1\n",
    "            elif event_order[i][m-1]==aligned_traces[i]['alignment'][j][0]: # log move detected\n",
    "                for q in range(m,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes from the current m to the maximum prefix length in this trace to 0 because deviation happened here but not afterwards\n",
    "                j-=1\n",
    "                m-=1\n",
    "            elif m==max_ev:\n",
    "                j-=1\n",
    "            else: # model move deteceted\n",
    "                for q in range(m+1,max_ev+1):\n",
    "                    y_cum_test[q][str(aligned_traces[i]['alignment'][j])][i]=0 # set all prefixes after the current m to the maximum prefix length in this trace to 0 because deviation happened between m and m+1 but not afterwards\n",
    "                j-=1\n",
    "        i+=1\n",
    "    ### y_cum_test holds information whether deviation happened after prefix length\n",
    "\n",
    "\n",
    "    ## ref_log will have all attributes that will be the columns for X_test and X_train\n",
    "    ref_log = complex_index_encoding(ref_log, 4000) # prepare a log with the maximum length of the feature vector from CIBE to know to pad other feature vectors\n",
    "    ref_dataframe1 = pm4py.convert_to_dataframe(ref_log)\n",
    "    ref_dataframe=ref_dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "    ref_dataframe=ref_dataframe.filter(like='case:', axis=1)\n",
    "    ref_dataframe=ref_dataframe.drop('case:concept:name', axis=1)\n",
    "    ref_dataframe.columns = ref_dataframe.columns.str.replace('case:', '')\n",
    "    ref_dataframe=ref_dataframe.reset_index()\n",
    "    ref_raw_dat=ref_dataframe.drop('index', axis=1)\n",
    "    ## dataset-specific preparation (i.e., redundant attributes, convertion to numeric)\n",
    "    if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "        ref_raw_dat['AMOUNT_REQ']= pd.to_numeric(ref_raw_dat['AMOUNT_REQ'])\n",
    "        ref_clean_dat=ref_raw_dat.drop('REG_DATE', axis=1)\n",
    "    elif z=='aligned_traces_20int.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20dom.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "    elif z=='aligned_traces_20prep.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "    elif z=='aligned_traces_20RfP.pkl':\n",
    "            ref_clean_dat=ref_raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "    else:\n",
    "        ref_clean_dat=ref_raw_dat.copy()\n",
    "    ref_enc_dat=pd.get_dummies(ref_clean_dat)\n",
    "\n",
    "\n",
    "    X_cum={}\n",
    "    metrics=pd.DataFrame(data=0, columns=dev, index=['Precision', 'Recall', 'Support', 'ROC_AUC','LenTrain', 'LenTrain_beforeUS_0', 'LenTrain_beforeUS_1', 'LenTrain_afterUS_0', 'LenTrain_afterUS_1'])\n",
    "\n",
    "    # prepare X for all prefix lengths\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        complex_index_encoding(log,prefix)\n",
    "        dataframe1 = pm4py.convert_to_dataframe(log)\n",
    "        dataframe=dataframe1.drop_duplicates(subset=['case:concept:name'])\n",
    "        dataframe=dataframe.filter(like='case:', axis=1)\n",
    "        dataframe=dataframe.drop('case:concept:name', axis=1)\n",
    "        dataframe.columns = dataframe.columns.str.replace('case:', '')\n",
    "        dataframe=dataframe.reset_index()\n",
    "        raw_dat=dataframe.drop('index', axis=1)\n",
    "        if z=='aligned_traces_12A.pkl' or z=='aligned_traces_12O.pkl' or z=='aligned_traces_12AO.pkl':\n",
    "            raw_dat['AMOUNT_REQ']= pd.to_numeric(raw_dat['AMOUNT_REQ'])\n",
    "            clean_dat=raw_dat.drop('REG_DATE', axis=1)\n",
    "        elif z=='aligned_traces_20int.pkl':\n",
    "            clean_dat=raw_dat.drop(['Permit travel permit number','DeclarationNumber','travel permit number','id','Permit ID', 'Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20dom.pkl':\n",
    "            clean_dat=raw_dat.drop(['DeclarationNumber','id'], axis=1)\n",
    "        elif z=='aligned_traces_20prep.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id','Permit travel permit number','Permit id'], axis=1)\n",
    "        elif z=='aligned_traces_20RfP.pkl':\n",
    "            clean_dat=raw_dat.drop(['RfpNumber','Rfp_id'], axis=1)\n",
    "        else:\n",
    "            clean_dat=raw_dat.copy()\n",
    "        enc_dat=pd.get_dummies(clean_dat)\n",
    "        for key in ref_enc_dat.columns:\n",
    "            if not key in enc_dat.columns:\n",
    "                enc_dat[key]=0 # pad all prefixes to maximum lengths with 0\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant',fill_value=0)\n",
    "        enc_dat=pd.DataFrame(data=imp.fit_transform(enc_dat),columns=enc_dat.columns)\n",
    "\n",
    "        X_cum[prefix]=enc_dat.copy()\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx)\n",
    "\n",
    "        X_cum[prefix]=X_cum[prefix].drop(drop_idx)\n",
    "\n",
    "    min_pref = 1\n",
    "    max_pref = max_ev\n",
    "    EPOCHS = 50\n",
    "    BATCH_SIZE = 128\n",
    "    LEARNING_RATE = 0.001\n",
    "\n",
    "    N = len(dev_df)\n",
    "\n",
    "    labels = dev  # ['label_1', ...., 'label_6']\n",
    "\n",
    "    positives = {}\n",
    "    negatives = {}\n",
    "    for label in labels:\n",
    "        positives[label] = sum(dev_df[label] == 1)\n",
    "        negatives[label] = sum(dev_df[label] == 0)\n",
    "    max_Plabel = max(positives.values())\n",
    "    max_Nlabel = max(negatives.values())\n",
    "    max_label = max(max_Plabel, max_Nlabel)\n",
    "    pir = {}\n",
    "    nir = {}\n",
    "    pirlbl = {}\n",
    "    nirlbl = {}\n",
    "    for label in labels:\n",
    "        pir[label] = max(positives[label], negatives[label]) / positives[label]\n",
    "        nir[label] = max(positives[label], negatives[label]) / negatives[label]\n",
    "        pirlbl[label] = max_label / positives[label]\n",
    "        nirlbl[label] = max_label / negatives[label]\n",
    "    positive_weights = {}\n",
    "    negative_weights = {}\n",
    "    for label in labels:\n",
    "        positive_weights[label] = mean(pir.values()) ** (1 / (4 * math.e)) + np.log(pirlbl[label])\n",
    "        negative_weights[label] = mean(nir.values()) ** (1 / (2 * math.e)) + np.log(nirlbl[label])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_train_idx, x_test_idx, y_train_idx, y_test_idx = train_test_split(range(len(log)), range(len(log)), test_size=split, random_state=0)\n",
    "\n",
    "    for d in dev:\n",
    "        metrics[str('NoDev'+d)]=0\n",
    "        metrics[d]['LenTrain_beforeUS_1']=sum(dev_df[d][i] for i in x_train_idx)\n",
    "        metrics[d]['LenTrain_beforeUS_0']=len(x_train_idx)-sum(dev_df[d][i] for i in x_train_idx)\n",
    "\n",
    "    if u_sample:\n",
    "        imb_ref_enc_dat = ref_enc_dat.copy()\n",
    "        imb_ref_enc_dat['ind'] = 0\n",
    "        for i in range(len(imb_ref_enc_dat)):\n",
    "            imb_ref_enc_dat['ind'][i] = i\n",
    "\n",
    "        imb_traces = pd.DataFrame(data=0, columns=['Dev'], index=range(len(log)))\n",
    "        for trace in range(len(log)):\n",
    "            if dev_df.loc[trace].sum() > 0:\n",
    "                imb_traces['Dev'][trace] = 1\n",
    "        imb_traces = imb_traces.drop(x_test_idx)\n",
    "        imb_ref_enc_dat = imb_ref_enc_dat.drop(x_test_idx)\n",
    "        imp = SimpleImputer(missing_values=np.nan, strategy='constant', fill_value=0)\n",
    "        imb_ref_enc_dat = pd.DataFrame(data=imp.fit_transform(imb_ref_enc_dat), columns=imb_ref_enc_dat.columns)\n",
    "        oss = OneSidedSelection(random_state=0, n_seeds_S=250, n_neighbors=7)\n",
    "        X_resampled, y_resampled = oss.fit_resample(imb_ref_enc_dat, imb_traces)\n",
    "        x_train_idx = list(X_resampled['ind'])\n",
    "        y_train_idx = list(X_resampled['ind'])\n",
    "\n",
    "    devq_df=dev_df.loc[x_train_idx]\n",
    "    print('index length ', len(x_train_idx),len(x_test_idx),len(y_train_idx),len(y_test_idx))\n",
    "    for d in dev:\n",
    "        metrics[d]['LenTrain']=len(x_train_idx)\n",
    "        metrics[d]['LenTrain_afterUS_1']=devq_df[d].sum()\n",
    "        metrics[d]['LenTrain_afterUS_0']=len(x_train_idx)-devq_df[d].sum()\n",
    "    # validation set for early stopping\n",
    "    x_train_idx, x_val_idx, y_train_idx, y_val_idx = train_test_split(x_train_idx, x_train_idx, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "    enumerated_trace_idx={}\n",
    "    for prefix in range(1,max_ev+1):\n",
    "        drop_idx=[]\n",
    "        for trace_idx in range(len(log)):\n",
    "            if event_count[trace_idx]< prefix:\n",
    "                drop_idx.append(trace_idx) # drop all trace encoding that do not go until prefix length\n",
    "\n",
    "        x_te=X_cum[prefix].loc[[j for j in list(set(y_test_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_tr=X_cum[prefix].loc[[j for j in list(set(y_train_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "        x_va=X_cum[prefix].loc[[j for j in list(set(y_val_idx)-set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_te = y_cum_test[prefix].loc[[j for j in list(set(y_test_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_va = y_cum_test[prefix].loc[[j for j in list(set(y_val_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        y_tr = y_cum_test[prefix].loc[[j for j in list(set(y_train_idx) - set(drop_idx))]].to_numpy().astype(float)\n",
    "        enumerated_trace_idx[prefix]=list(set(y_test_idx)-set(drop_idx))\n",
    "        print('subset length ',prefix, len(x_te),len(x_tr),len(y_te),len(y_tr))\n",
    "\n",
    "        if prefix ==1:\n",
    "            X_train = x_tr\n",
    "            X_test = x_te\n",
    "            y_train = y_tr\n",
    "            y_test = y_te\n",
    "            X_val = x_va\n",
    "            y_val = y_va\n",
    "        else:\n",
    "            X_train = np.append( X_train, x_tr, axis=0)\n",
    "            X_test = np.append( X_test, x_te, axis=0)\n",
    "            y_train = np.append( y_train, y_tr, axis=0)\n",
    "            y_test = np.append( y_test, y_te, axis=0)\n",
    "            y_val = np.append( y_val, y_va, axis=0)\n",
    "            X_val = np.append( X_val, x_va, axis=0)# combine all X data from all prefixes into one array\n",
    "    print(d, len(X_train),len(y_train),len(X_val),len(y_val),len(X_test),len(y_test))\n",
    "\n",
    "    print('split done')\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    X_val = scaler.fit_transform(X_val)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BinaryClassification(no_columns=len(ref_enc_dat.loc[0]), no_devs=len(dev))\n",
    "    model.to(device)\n",
    "    criterion = nn.MultiLabelSoftMarginLoss(weight=torch.FloatTensor(list(positive_weights.values())))\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    if not early_stop:\n",
    "\n",
    "        print('training start no ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        for e in range(1, EPOCHS + 1):\n",
    "            epoch_loss = 0\n",
    "            epoch_acc = 0\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                y_pred = model(X_batch)\n",
    "\n",
    "                loss = criterion(y_pred, y_batch)\n",
    "                acc = binary_acc(y_pred, y_batch) / len(y_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "\n",
    "            print(\n",
    "                f'Epoch {e + 0:03}: | Loss: {epoch_loss / len(train_loader):.5f} | Acc: {epoch_acc / len(train_loader):.3f} |',\n",
    "                prefix)\n",
    "\n",
    "    else:\n",
    "        print('training start with ES')\n",
    "        model.train()\n",
    "        train_data = TrainData(torch.FloatTensor(X_train),\n",
    "                               torch.FloatTensor(y_train))\n",
    "\n",
    "        train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "        X_val = torch.FloatTensor(X_val)\n",
    "\n",
    "        es = EarlyStopping()\n",
    "        done = False\n",
    "\n",
    "        epoch = 0\n",
    "        while epoch < EPOCHS and not done:\n",
    "            epoch += 1\n",
    "            steps = list(enumerate(train_loader))\n",
    "            pbar = tqdm.tqdm(steps)\n",
    "            model.train()\n",
    "            epoch_acc = 0\n",
    "            epoch_loss = 0\n",
    "            for i, (x_batch, y_batch) in pbar:\n",
    "                optimizer.zero_grad()\n",
    "                y_batch_pred = model(x_batch.to(device))\n",
    "\n",
    "                #print(y_batch_pred.shape)\n",
    "\n",
    "                loss = criterion(y_batch_pred, y_batch.to(device))\n",
    "\n",
    "                acc = binary_acc(y_batch_pred, y_batch.to(device)) / len(y_batch_pred[0])\n",
    "\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_acc += acc.item()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "                current = (i + 1) * len(x_batch)\n",
    "                if i == len(steps) - 1:\n",
    "                    model.eval()\n",
    "                    pred = model(X_val)\n",
    "                    vloss = criterion(pred, torch.FloatTensor(y_val))\n",
    "                    if es(model, vloss): done = True\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss: {epoch_loss / len(train_loader)}, Acc: {epoch_acc / len(train_loader):.3f}, vloss: {vloss:>7f}, EStop:[{es.status}]\")\n",
    "                else:\n",
    "                    pbar.set_description(\n",
    "                        f\"Epoch: {epoch}, tloss {epoch_loss / len(train_loader):}, Acc: {epoch_acc / len(train_loader):.3f}\")\n",
    "\n",
    "    model.eval()\n",
    "    test_data = TestData(torch.FloatTensor(X_test))\n",
    "    test_loader = DataLoader(dataset=test_data, batch_size=1)\n",
    "\n",
    "    y_pred_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            y_test_pred = model(X_batch)\n",
    "            #y_test_pred = torch.sigmoid(y_test_pred)\n",
    "            y_pred_tag = torch.round(torch.sigmoid_(y_test_pred))\n",
    "            y_pred_list.append(y_pred_tag.cpu().numpy())\n",
    "\n",
    "    y_pred_list = [a.squeeze().tolist() for a in y_pred_list]\n",
    "\n",
    "    CM = sklearn.metrics.multilabel_confusion_matrix(y_test, y_pred_list)\n",
    "\n",
    "    for i in range(len(dev)):\n",
    "        metrics[str('NoDev' + dev[i])] = 0\n",
    "        metrics[dev[i]]['Precision'] = CM[i][1][1] / (CM[i][1][1] + CM[i][0][1])\n",
    "        metrics[dev[i]]['Recall'] = CM[i][1][1] / (CM[i][1][1] + CM[i][1][0])\n",
    "        try:\n",
    "            metrics[dev[i]]['ROC_AUC'] = sklearn.metrics.roc_auc_score(y_test[:,i], np.array(y_pred_list)[:,i], average='macro')\n",
    "        except Exception as er:\n",
    "            metrics[dev[i]]['ROC_AUC'] = er\n",
    "        metrics[str('NoDev' + dev[i])]['Precision'] = CM[i][0][0] / (CM[i][0][0] + CM[i][1][0])\n",
    "        metrics[str('NoDev' + dev[i])]['Recall'] = CM[i][0][0] / (CM[i][0][0] + CM[i][0][1])\n",
    "\n",
    "\n",
    "\n",
    "    path = (os.getcwd() + '/BPDP_Classifier')\n",
    "    xt, z = os.path.split(file)\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        path + '/' + z + '_BPDP_single_classifier_testcounts' + str(early_stop) + '_' + str(round(split, 2)) + '.xlsx',\n",
    "        engine=\"xlsxwriter\")\n",
    "\n",
    "    metrics.to_excel(writer, sheet_name=('Metrics'))\n",
    "    writer.close()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "IDP_collective_CIBE(log, ref_log, aligned_traces, u_sample=True, split=1/3, early_stop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ]
}
